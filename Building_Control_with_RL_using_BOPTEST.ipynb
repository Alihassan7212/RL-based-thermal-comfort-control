{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alihassan7212/RL-based-thermal-comfort-control/blob/main/Building_Control_with_RL_using_BOPTEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1CcDG8FanTw"
      },
      "source": [
        "# **Key Learning Objectives** üéØ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT2QjTu24zwV"
      },
      "source": [
        "\n",
        "This is an introductory, hands-on tutorial to guide you through the main concepts of Reinforcement Learning (RL) for controlling Heating, Ventilation and Air Conditioning (HVAC) systems for buildings.\n",
        "We are going to apply RL to a building emulator from the Building Optimization Testing (BOPTEST) framework **[1]** using the BOPTEST-Gym interface **[2]**.\n",
        "BOPTEST is a framework for performance benchmarking of control algorithms.\n",
        "Further information and documentation can be found here:\n",
        "\n",
        "[https://ibpsa.github.io/project1-boptest/](https://ibpsa.github.io/project1-boptest/)\n",
        "\n",
        "You will learn:\n",
        "\n",
        "- What RL is, how it works and how it can be used in the application of building energy management.\n",
        "- The most popular standard for representing general RL problems: Gymnasium.\n",
        "- The BOPTEST API and its Gym interface.\n",
        "\n",
        "üìå **Note**: This tutorial is prepared for use with BOPTEST v0.6.0.\n",
        "and uses a web-based version of BOPTEST (called \"BOPTEST-Service\") as not to require installation of any BOPTEST software on a user's own device. It is also possible to use BOPTEST on a user's own (local) device.\n",
        "Both the web-based and local versions have the same functionality, and will produce the same results, with only small changes in the API (changing the BOPTEST-service url to your localhost url, that is, to: `http://127.0.0.1:5000/<request>`). The tutorial is continuously updated to work with the latest BOPTEST versions. See [the release notes](https://github.com/ibpsa/project1-boptest-gym/blob/master/releasenotes.md) for the version history.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSTpxm2GrjhR"
      },
      "source": [
        "# **Outline** ‚è∞\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUbaQ5GqrvIl"
      },
      "source": [
        "[Part 1: Background](#background)\n",
        "  1. [Introduction to Reinforcement Learning](#introRL)\n",
        "  1. [Application of Reinforcement Learning in buildings](#applicationRL)\n",
        "\n",
        "[Part 2: The Gymnasium standard](#Gymnasium)\n",
        "  1. [What is Gymnasium?](#whatIsGymnasium)\n",
        "  1. [Example using a Gymnasium environment](#exampleGymnasium)\n",
        "  1. [üí• Exercise 1: improve Gymnasium environment's controller](#exerciseGymnasium)\n",
        "\n",
        "[Part 3: The Building Optimization Testing (BOPTEST) Framework](#boptest)\n",
        "  1. [What is BOPTEST?](#whatIsBoptest)\n",
        "  1. [Selecting a building test case](#selectBuilding)\n",
        "  1. [Obtaining general information about the building](#obtainInfo)\n",
        "  1. [Getting control input and measurement points](#gettingIOs)\n",
        "  1. [üí• Exercise 2: implement a simple controller in BOPTEST](#exerciseBoptest)\n",
        "\n",
        "[Part 4: Implementing RL for a building with BOPTEST-Gym](#implementingRL)\n",
        "  1. [What is BOPTEST-Gym?](#whatIsBoptestGym)\n",
        "  1. [Starting up a BOPTEST-Gym environment](#startingUpBoptestGym)\n",
        "  1. [Interacting with a BOPTEST-Gym environment](#interactingWithBoptestGym)\n",
        "  1. [üí• Exercise 3: implement a simple controller in BOPTEST-Gym](#exerciseBoptestGym)\n",
        "  1. [Developing a basic RL algorithm](#developingRlAlgo)\n",
        "  1. [Testing our RL algorithm in BOPTEST-Gym](#testingRlAlgo)\n",
        "  1. [üí• Exercise 4: expedite learning in BOPTEST-Gym](#exerciseBoptestGymExpedite)\n",
        "\n",
        "[Gearing up](#gearingUp)\n",
        "\n",
        "[Further resources](#furtherResources)\n",
        "\n",
        "[Feedback](#feedbackForm)\n",
        "\n",
        "[Annex I: Formal RL theory](#theoryRlFormal)\n",
        "\n",
        "[References](#references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEzP9ZW4MXPv"
      },
      "source": [
        "# **Part 1: Background** üìñ <a name=\"background\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fas232CyMX6_"
      },
      "source": [
        "## **Introduction to Reinforcement Learning** <a name=\"introRL\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAy9fRjUTSdb"
      },
      "source": [
        "Could you imagine a magic oracle able to decide on the best actions to optimize any process? Could you imagine this oracle not needing any prior information of the process but just learning from interacting with it? Powerful, right? Well, that is exactly what RL is meant for.\n",
        "\n",
        "Reinforcement Learning (RL) is one of the categories of machine learning, along with unsupervised learning and supervised learning. The main difference from the other categories is that RL learns from dynamic data, that is, data that are obtained while learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e853vYumSx08"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1w0zsvPXdy0bR67T6xJupyYUg5IM1vaYR\" width=\"500\"/>\n",
        "\n",
        "*Figure: The categories of machine learning. Source: [Mathworks](https://www.mathworks.com/discovery/reinforcement-learning.html)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_qdE6Ab4aE9"
      },
      "source": [
        "In RL the goal is to learn the actions to be taken to achieve a predefined objective. RL relies on the principle of *repetitive experimentation*, that is, an approach where we roll out several **episodes of experience** where an agent ü§ñ (the RL algorithm) interacts with its environment üåé (the process to be optimized) to learn based on a **reward** signal that is returned for every **action** taken from a specific **state** of the environment.\n",
        "\n",
        "Let's take the example of teaching a dog to grab a stick. In this case, the dog is the agent and all its surroundings conform the environment. Whenever the dog observes that there is a person throwing a stick it will perform an action. In case it grabs the stick and brings it back, the person will provide a cookie as a reward to encourage that behavior. In case the dog does not go for the stick but just runs around or goes chasing other dogs, the person will not provide the cookie. Eventually, the dog will associate the actions that bring the most rewards to specific observations and will be taking those actions accordingly.\n",
        "\n",
        "A more formal introduction to RL and its associated terminology can be found at the end of this tutorial, in [Annex I: Formal RL theory](#theoryRlFormal)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMppgppKX4Fy"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1Leqg1LIT4LRv84B6OVG51O4QSOXvfw0Y\" width=\"500\"/>\n",
        "\n",
        "*Figure: RL notation when teaching a dog. Source: [Mathworks](https://www.mathworks.com/discovery/reinforcement-learning.html)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNy7uRzo8foI"
      },
      "source": [
        "‚ö†Ô∏è **Important note:** ‚ö†Ô∏è It is common to find in the RL literature that the same term indistinctly designates the\n",
        "state and the observation. This is not strictly correct for partially observable environments (most of the cases) where the observation only conveys part of the information that defines the state. For example, the state of the Tic-Tac-Toe game can be fully observed because there is a finite number of possibilities that define the state of a game. On the contrary, the thermal state of a building is only partially oservable. We can observe the indoor air temperature, but we cannot measure all temperatures from walls, ground, furniture... which also influence the building's thermal state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNC0UnC2WYyE"
      },
      "source": [
        "\n",
        "\n",
        "What is particularly extraordinary of RL algorithms is that the same algorithm can be successfully used for a variety of tasks, from [robotic motion control](https://www.technologyreview.com/2021/04/08/1022176/boston-dynamics-cassie-robot-walk-reinforcement-learning-ai/) to [defeat the human world champion in the game of Go](\n",
        "https://www.youtube.com/watch?v=WXuK6gekU1Y&ab_channel=DeepMind).\n",
        "The latter is an astonishing achievement. It is true that the IBM supercomputer Deep Blue could previously [defeat Garry Casparov in chess](https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov), but Go is to chess what chess is to Tic-Tac-Toe ([*Chris Wiltz*](https://www.designnews.com/design-hardware-software/googles-ai-beat-go-champion-mimicking-human-intuition)). And what is more important, professionals of Go state that this game has so many possible combinations that mastering it requires certain intuition and creativity, qualities that have only been attributed to humans so far... if AlphaGo defeated the best human player of Go, could machines resemble these qualities? Well, that is more a philosophical question. This tutorial is limited to investigate whether machines can efficiently control buildings, which you will see is already an enormous challenge!\n",
        "\n",
        "<!-- ###**MAIN CHALLENGES OF RL:**\n",
        "- **Curse of dimensionality**: to be practical, an RL agent needs to be sample efficient. That means that it has to learn from a reasonable amount of interactions with the environment. Note that an untrained agent is like a newborn baby that can only learn from rewards of the environment.  \n",
        "- The feedback obtained from the environment may be **delayed**, not instantaneous. The agent should be able to learn that the consequence of an action may be well dalayed in time.\n",
        "- The function approximators used to represent the agent's policies are trained with **non-independent and identically distributed (i.i.d.) data**, assumption that is typically taken to parametrize policies and value functions.  -->\n",
        "\n",
        " <!--\n",
        "Different families of methods exist to address MDPs. However, Reinforcement Learni RL is the one that\n",
        "can tackle systems that involve continuous variables without assuming perfect\n",
        "system knowledge. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOcvaJpkNbho"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1-r7O-0ROh0cXs6qxatWXztf_lvr5jcCL\" width=\"500\"/>\n",
        "\n",
        "*Figure: Netflix documentary that explains how AlphaGo, a RL algorithm developed by [DeepMind](https://www.deepmind.com/), could defeat Lee Sedol (4-1) and Fan Hui (5-0), the human world champions in the game of Go.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1jhKBOeOciu"
      },
      "source": [
        "üìå **QUICK FACTS:**\n",
        "- RL is a **category of machine learning** algorithms, together with supervised and unsupervised learning.\n",
        "- Contrarily to other machine learning techniques, RL learns from **dynamic data**, that is, data that are obtained from interactions with the environment.\n",
        "- Particularly, it learns from **state-action-reward** samples, so there is no need of domain knowledge to model the environment.\n",
        "\n",
        "<!-- - The counterpart of RL in the control theory community is **Model Predictive Control (MPC)**. Although both methods pursue the same goal (optimal control) they follow very different approaches and use different terminologies.\n",
        "- In essence, the main difference between MPC and RL is that MPC requires a **model** of the environment, whereas RL can just learn from interactions with it. On the downside, RL needs a lot of data for learning, has difficulties dealing with constraints, and lacks intelligibility.  -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G1nWECgbmuW"
      },
      "source": [
        "## **Application of Reinforcement Learning in buildings** <a name=\"applicationRL\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoTh8XvAM_OR"
      },
      "source": [
        "During the last decade, there has been a clear interest growth in using optimal control for HVAC systems **[3]**. The figure below underlines this increased interest by showing the number of yearly peer-reviewed scientific publications related to optimal control in buildings.\n",
        "RL algorithms have\n",
        "gained particular popularity for their application in a **demand response** setting.\n",
        "An extensive review for this application was written by ¬†V√°zquez-Canteli et al. **[4]** This review is\n",
        "not limited to HVAC systems but also demand response for charging electric\n",
        "vehicles or thermal energy storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NywyXo6hD5n"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1-rfj3nQ7MIv8Vgfb8iwESATdB9lc2vmv\" width=\"500\"/>\n",
        "\n",
        "*Figure: Evolution of the number of scientific publications about optimal control in buildings during the last decades. Data obtained from the Clarivate Web of Science.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KR-sTeJiJG2"
      },
      "source": [
        "RL has already attracted the attention of the building control community for\n",
        "many years. The figure below is obtained from the popular paper of Chen et al. **[5]** who graphically summarized the application of RL in buildings indicating the amount of data required by each research work to train the implemented RL algorithm.\n",
        "\n",
        "<!-- Only recently synergetic methods are gaining attention.  -->\n",
        "\n",
        "<!-- As explained before, there are two main families of methods that can tackle large and complex systems comprising several variables like buildings: MPC and RL. Their parallel development is also reflected in their applicability in buildings with authors typically adopting either one or the other approach.  -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCH0DBBbPEC7"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1-vJiJ63O9pHklURqNgWxIHqMZQjMgJKK\" width=\"700\"/>\n",
        "\n",
        "\n",
        "*Figure: Summary of the data required in the history of RL applications to buildings. Chen et al.* **[5]** ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae18iXNKWV5I"
      },
      "source": [
        "You can see from the figure that the feasibility and potential of applying RL for HVAC control\n",
        "was first investigated by Liu and Henze back in 2006. Then, the interest was lost for a period, probably because Model Predictive Control (MPC) has been typically preferred for optimal control in buildings because it is much more data-efficient (it does not need as much data to be implemented). A comprehensive and complete review on the application of MPC for building energy management is provided by Drgona et al. **[6]**.    \n",
        "The reasons why RL is gaining momentum again are clear:  \n",
        "\n",
        "- Evolution in deep learning\n",
        "- We have much more data than before\n",
        "- We have much more computational power than before\n",
        "\n",
        "In fact, there exist very recent developments for the application of RL in buildings, most of them using the Gymnasium standard that is introduced in the next section. It is worth mentioning:\n",
        "\n",
        "- [CityLearn](https://github.com/intelligent-environments-lab/CityLearn) ‚û°Ô∏è Gym environment for providing demand response scenarios at an urban scale. That is, the goal of the RL agent is to flatten the energy demand of a district. It considers static\n",
        "building heating and cooling load data and simplified models for energy storage.\n",
        "- [Gym-Eplus](https://github.com/zhangzhizza/Gym-Eplus) ‚û°Ô∏è Gym environment wrapper around EnergyPlus simulation models.\n",
        "- [Sinergym](https://github.com/ugr-sail/sinergym) ‚û°Ô∏è Extension of Gym-Eplus.\n",
        "- [Energym](https://github.com/bsl546/energym) ‚û°Ô∏è Gym wrapper around building simulation models to assess controller performance.\n",
        "- [Beobench](https://github.com/rdnfn/beobench) ‚û°Ô∏è A Toolkit for Unified Access to BuildingSimulations for Reinforcement Learning.\n",
        "- üëâüèª[BOPTEST-Gym](https://github.com/ibpsa/project1-boptest-gym) ‚û°Ô∏è Gym environment for the BOPTEST Framework. The goal of the RL agent in this environment is to efficienty control an individual building. It allows testing against high-fidelity building models.\n",
        "\n",
        "The last of which is the focus of this tutorial.\n",
        "\n",
        "These RL frameworks for HVAC control bring hope\n",
        "to the adoption of this technology in buildings. However, there is still a clear\n",
        "need to different techniques and understand the best practices of RL\n",
        "for this particular application. Let's investigate how!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YnuNAQdM_L2"
      },
      "source": [
        "# **Part 2: The Gymnasium standard** ü§ñ <a name=\"Gymnasium\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv728rc3M_Ir"
      },
      "source": [
        "## **What is Gymnasium?** <a name=\"whatIsGymnasium\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhQfyrBCUigq"
      },
      "source": [
        "[Gymnasium](https://gymnasium.farama.org/) is a software library that gathers a **collection of tasks** called environments with a **unique Python interface** to control all of them. This unique interface is a key feature in the software package, and has given rise to a standard for the format in which RL agents are developed and treated, independently of\n",
        "their internal functioning. The tasks defined in the Gym environments involve\n",
        "a wide variety of fields like video games, classic control theory problems, or high dimensional robotic locomotive tasks.\n",
        "You can find a list of available Gymnasium environments [here](https://gymnasium.farama.org/environments/classic_control/).\n",
        "\n",
        "<!-- There also exist numerous third party environments aiming to engage control developers to test\n",
        "their algorithms in their field of interest, e.g. for the optimization of electrical mircrogrids or for the intelligent control of electric vehicles. -->\n",
        "\n",
        "The Gymnasium philosophy heavily relies on the episodic aspect of RL, i.e.\n",
        "the agent‚Äôs history is broken down into a series of experiences called **episodes** that may be of\n",
        "variable length. The agent interacts with the environment until it reaches a\n",
        "terminal state when the episode is finished. The goal is to maximize the total\n",
        "cumulative reward per episode.\n",
        "\n",
        "The main methods of the Gymnasium interface are the following:\n",
        "\n",
        "- `obs = env.reset()` ‚û°Ô∏è The `reset` method is the one called first to initialize the environment `env` (whatever it is). The environment returns the first observation `obs` (state).\n",
        "- `next_obs,reward,terminated,truncated,info = env.step(action)` ‚û°Ô∏è The `step` method is used iteratively to interact with the environment. The RL agent computes an `action`, and the environment returns the next observation, associated reward, whether the episode finishes because the goal has been reached (=terminated), whether the episode is artificially finished (=truncated), and some other optional information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntNSOBzJPJuF"
      },
      "source": [
        "## **Example using a Gymnasium environment** <a name=\"exampleGymnasium\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ2879zIPOJ0"
      },
      "source": [
        "Now that we understand the main concepts of Gymnasium we are going to illustrate its typical usage with a quick example. We're going to use the [CartPole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/), which is one of the classic control problems available in the Gymnasium framework.\n",
        "Let's start by installing the dependencies that we require:\n",
        "\n",
        "<!-- !pip3 install gym==0.21.0 stable-baselines3==0.8.0 torch==1.11.0 -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "E0sfte45O8iN",
        "outputId": "2b730b5f-c3f8-42d7-a641-f1c0dbe69d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium==0.28.1 in /usr/local/lib/python3.12/dist-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.28.1) (1.26.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.28.1) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.28.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.28.1) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.28.1) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium==0.28.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnmDsSAnM_F-"
      },
      "source": [
        "**Cartpole environment description:**\n",
        "\n",
        "\"*A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left (-1) and right (+1) direction on the cart. A reward of +1 is provided for every timestep that the pole remains upright.*\"\n",
        "\n",
        "\n",
        "See below an example of the evolution of an episode of the Cartpole environment. Note that most of the Gym envronments can be rendered to show how the RL agent is performing.\n",
        "\n",
        "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMa2F3q_pV-C"
      },
      "source": [
        "First, we are going to import `gym` and then `make` our Cartpole environment (version 1). Note how it is possible to instantiate the registered Gym environments by referring to their names with a string.\n",
        "After that, we are going to interact with the environment for a maximum number of episodes of experience that we are going to indicate with `max_num_episodes`. In each episode, the environment is initialized with the `reset` method, and then we interact with the environment until the episode is `done`, which happens when the pole is down.\n",
        "\n",
        "It is important to note that in this example we are not implementing RL just yet. Instead, we are using the `sample` method from the action space of the environment to compute a random control action. This is useful when we want to quickly check how an environment behaves, but we should aim to replace that line by some intelligent RL agent able to compute a control action that optimizes the performance of the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LBxXhZc5nGb3",
        "outputId": "5eb967d9-ca5b-427f-f532-0a01390239ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Episode #1 had 16 steps and total_reward=16.0\n",
            "\n",
            " Episode #2 had 10 steps and total_reward=10.0\n",
            "\n",
            " Episode #3 had 15 steps and total_reward=15.0\n",
            "\n",
            " Episode #4 had 26 steps and total_reward=26.0\n",
            "\n",
            " Episode #5 had 10 steps and total_reward=10.0\n",
            "\n",
            " Episode #6 had 20 steps and total_reward=20.0\n",
            "\n",
            " Episode #7 had 69 steps and total_reward=69.0\n",
            "\n",
            " Episode #8 had 21 steps and total_reward=21.0\n",
            "\n",
            " Episode #9 had 18 steps and total_reward=18.0\n",
            "\n",
            " Episode #10 had 12 steps and total_reward=12.0\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "max_num_episodes = 10 # maximum number of episodes\n",
        "\n",
        "for episode in range(max_num_episodes):\n",
        "  done = False\n",
        "  obs, _ = env.reset()\n",
        "  total_reward = 0.0\n",
        "  step = 0\n",
        "  while not done:\n",
        "    action = env.action_space.sample() # Compute random action. This is to be replaced by a control logic or RL algorithm\n",
        "    obs,reward,terminated,truncated,info = env.step(action) # send the action to the environment\n",
        "    done = (terminated or truncated)\n",
        "    total_reward += reward\n",
        "    step += 1\n",
        "\n",
        "  print('\\n Episode #{} had {} steps and total_reward={}'.format(episode+1,step,total_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai9dHGWksZeu"
      },
      "source": [
        "Notice how every episode lasts for a different number of steps because we are applying random forces to the cart. Also, notice how the total reward of each episode is equal to the number of steps, because the Cartpole environment gives a reward of +1 every timestep that we get to maintain the pole upright.\n",
        "\n",
        "<!-- *Note: this part of the tutorial is mostly inspired in [this notebook](https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/1_getting_started.ipynb#scrollTo=pUWGZp3i9wyf) provided in the Stable Baselines repository. Feel free to check it out if you have questions!* -->\n",
        "\n",
        "<!-- import gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo.policies import MlpPolicy\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "model = PPO(MlpPolicy, env, verbose=0)\n",
        "\n",
        "def evaluate(model, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_episodes: (int) number of episodes to evaluate it\n",
        "    :return: (float) Mean reward for the last num_episodes\n",
        "    \"\"\"\n",
        "    # This function will only work for a single Environment\n",
        "    env = model.get_env()\n",
        "    all_episode_rewards = []\n",
        "    for i in range(num_episodes):\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        while not done:\n",
        "            # _states are only useful when using LSTM policies\n",
        "            action, _states = model.predict(obs)\n",
        "            # here, action, rewards and dones are arrays\n",
        "            # because we are using vectorized env\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
        "\n",
        "    return mean_episode_reward\n",
        "\n",
        "# Random Agent, before training\n",
        "mean_reward_before_train = evaluate(model, num_episodes=100)\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "# Train the agent for 10000 steps\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\") -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1VGo-pvB67F"
      },
      "source": [
        "##  üí• **Exercise 1: improve Gymnasium environment's controller** <a name=\"exerciseGymnasium\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf8KjgSbB67G"
      },
      "source": [
        "The previous example led to very small total cummulative rewards because of the silly random controller that was implemented. We could easily improve this controller by applying simple logic to the action to be taken instead of just using a random action. Try to increase the average time the pendulum is upright by deciding on the force to be applied (either left `action = 0` or right `action = 1`) using any of the metrics that are returned in the observation vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9MEjnCdbB67H",
        "outputId": "abb5532b-03bc-4cf2-ac0e-bf8e4900945d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Episode #1 had 38 steps and total_reward=38.0\n",
            "\n",
            " Episode #2 had 62 steps and total_reward=62.0\n",
            "\n",
            " Episode #3 had 52 steps and total_reward=52.0\n",
            "\n",
            " Episode #4 had 40 steps and total_reward=40.0\n",
            "\n",
            " Episode #5 had 34 steps and total_reward=34.0\n",
            "\n",
            " Episode #6 had 56 steps and total_reward=56.0\n",
            "\n",
            " Episode #7 had 66 steps and total_reward=66.0\n",
            "\n",
            " Episode #8 had 47 steps and total_reward=47.0\n",
            "\n",
            " Episode #9 had 53 steps and total_reward=53.0\n",
            "\n",
            " Episode #10 had 36 steps and total_reward=36.0\n"
          ]
        }
      ],
      "source": [
        "# Define the control algorithm\n",
        "def control_algorithm(obs):\n",
        "\n",
        "    # Extract the relevant variables from the observation\n",
        "    cart_position = obs[0]\n",
        "    cart_velocity = obs[1]\n",
        "    pole_angle = obs[2]\n",
        "    pole_velocity = obs[3]\n",
        "\n",
        "    if pole_angle < 0:\n",
        "      action = 0  # Push the cart to the left\n",
        "    else:\n",
        "      action = 1  # Push the cart to the right\n",
        "    return action\n",
        "\n",
        "# Run the control algorithm for a specified number of episodes\n",
        "max_num_episodes = 10\n",
        "for episode in range(max_num_episodes):\n",
        "    done = False\n",
        "    obs, _ = env.reset()\n",
        "    total_reward = 0.0\n",
        "    step = 0\n",
        "    while not done:\n",
        "        # Part 2\n",
        "        action = action = control_algorithm(obs)\n",
        "        obs,reward,terminated,truncated,info = env.step(action) # send the action to the environment\n",
        "        done = (terminated or truncated)\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "\n",
        "    print('\\n Episode #{} had {} steps and total_reward={}'.format(episode+1, step, total_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ISZahhAB67O"
      },
      "source": [
        "You should see a print indicating the number of steps per episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpeeOA8BM-5L"
      },
      "source": [
        "# **Part 3: The Building Optimization Testing (BOPTEST) framework** üè† <a name=\"boptest\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0ry1NQwuMXa"
      },
      "source": [
        "Now that we understand how RL and Gymnasium work, let's use that knowledge for the particular application of buildings.\n",
        "In this tutorial we are going to connect with a BOPTEST building emulator that we will use as our environment to control through RL.\n",
        "This emulator is a simulation model that was configured based on detailed physics and that has been peer-reviewed to ensure that it represents the behavior of an actual building as realistically as possible. Hence, although it is a simulation model, we are going to consider this emulator as the real building for control, testing and benchmarking.\n",
        "\n",
        "In this section we explain what BOPTEST is and how it can be generally used. Next section will exclusively focus on BOPTEST-Gym, the Gymnasium interface of BOPTEST, to learn how we can use it to implement and assess RL algorithms for building control."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcPk7llkJP4m"
      },
      "source": [
        "## **What is BOPTEST?** <a name=\"whatIsBoptest\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtDrzTFuJU0e"
      },
      "source": [
        "BOPTEST is a software framework enables the performance evaluation and benchmarking of advanced control algorithms for building HVAC control through simulations. The software is developed and is available on the BOPTEST GitHub respository at [https://github.com/ibpsa/project1-boptest](https://github.com/ibpsa/project1-boptest)\n",
        "\n",
        "and general information about BOPTEST can be found through the following link:\n",
        "\n",
        "[https://ibpsa.github.io/project1-boptest/](https://ibpsa.github.io/project1-boptest/)\n",
        "\n",
        "In the link below you can also find information about the overarching project that gave birth to BOPTEST, IBPSA Project 1:\n",
        "\n",
        "[https://ibpsa.github.io/project1/](https://ibpsa.github.io/project1/)\n",
        "\n",
        "\n",
        "<!-- The main advantage of using the BOPTEST framework as our system is that we can use building models readily available in a server to generate data and to interact with them. Each BOPTEST emulator comes along with a baseline controller that is representative of currently typical HVAC control practice of the considered building case.  -->\n",
        "\n",
        "<!-- Therefore, to generate sample data for our forecasting exercise we are going to connect to a BOPTEST building test case emulator, simulate the model for a long period, and store the data points that we are going to use as features and targets of our machine learning algorithms. Note that BOPTEST will only expose data points that could be measured in practice.  -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNiHr2w0IFYI"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1XVbDEiHT2fWIGtnPLE0uphC2hV5XubKc\" width=\"500\"/>\n",
        "\n",
        "*Figure: The BOPTEST concept.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj7kbbF8JXEG"
      },
      "source": [
        "The main use case of the BOPTEST framework is the one where a control developer wants to evaluate the performance of his/her building control strategy. Testing in a real building may be very expensive, or just not possible. BOPTEST offers a menu of emulator building models so that the control developer can select one of them, interact in co-simulation, and eventually assess the performance of his/her controller with a set of Key Performance Indicators (KPIs) that are calculated by the BOPTEST framework.   \n",
        "\n",
        "Note that using a standardized set of building emulators, testing scenarios, and KPIs enables benchmarking, allows to compare across different controllers, and throws light on what are the best practices for building control.  In addition, making these emulators easily and rapidly available to use allows for control developers without expertise in building modeling to utilize them for controls testing and evaluation.\n",
        "\n",
        "In this section we are going to explain the basic BOPTEST functionality to connect to a building test case and obtain available control inputs and measurement points. For a more complete description on how to use BOPTEST please visit this [BOPTEST Colab tutorial](https://github.com/ibpsa/project1-boptest/blob/master/docs/workshops/BS21Workshop_20210831/Introduction_to_the_BOPTEST_framework.ipynb).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Fgw7eJHEjy"
      },
      "source": [
        "## **Selecting a building test case** <a name=\"selectBuilding\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owb2Z2rqHEjz"
      },
      "source": [
        "BOPTEST test cases are developed as [Functional Mock-up Units (FMU's)](https://fmi-standard.org/) and deployed within a containerized environment using the [Docker](https://www.docker.com/) software with:\n",
        "\n",
        "*   A detailed emulator **building model**.\n",
        "*   Yearly **boundary condition data** for weather, schedules, pricing, and emission factors. These data are representative of the building location.\n",
        "*   An **API** that allows for, among other things, initializing a simulation or testing scenario, advancing a simulation with a control input, receiving forecast data, receiving emulator data, and receiving computed KPIs.  The full API is described [here](https://github.com/ibpsa/project1-boptest/tree/boptest-service#test-case-restful-api).\n",
        "\n",
        "The basic workflow to test a controller is:\n",
        "\n",
        "1.   Select a **test case** from the menu of those available.\n",
        "2.   Select one of the **testing scenarios** defined for the given test case. Testing scenarios are standardized for each emulator.\n",
        "3.   Set **parameters** for the interaction with your test controller, such as the control step or forecast horizon, if required.  \n",
        "4.   Run the test case scenario in a **co-simulation** loop with your controller.\n",
        "5.   Request the KPIs and **evaluate** your controller's performance.\n",
        "\n",
        "We start by selecting and launching a BOPTEST building test case from the [repository of currently available test cases](https://ibpsa.github.io/project1-boptest/testcases/index.html).  In this example, we are going to work with the test case called `bestest_hydronic_heat_pump`, which is a single-zone residential building with radiant floor heating and a heat pump. This is a high-fidelity, yet, relatively simple test case that allows us to focus on fundamental aspects.  You may want to note the other test cases available in the repository as well as the fact that there are more under development.  \n",
        "\n",
        "We can launch our chosen test case as follows.  First, import the Python `requests` library so that we can make HTTP requests to the BOPTEST API at the address indicated by the `url`.  Then, use the `POST /testcases/<test_case_name>/select` BOPTEST API endpoint to launch the test case and receive a corresponding `testid`.  While the `url` is the common gateway for everyone to access the BOPTEST web-service, the `testid` is a unique identifier for you to address the test case that you have selected and launched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V_qU6ukZghTb"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# url for the BOPTEST service\n",
        "url = 'https://api.boptest.net'\n",
        "\n",
        "# Select test case and get identifier\n",
        "testcase = 'bestest_hydronic_heat_pump'\n",
        "\n",
        "# Select and start a new test case\n",
        "testid = \\\n",
        "requests.post('{0}/testcases/{1}/select'.format(url,testcase)).json()['testid']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRZGKWDlHEj2"
      },
      "source": [
        "Once you have successfully obtained the `testid`, it is possible to start interacting with your selected test case using the rest of the BOPTEST API.  You will need this `testid` for all further interactions with this test case.  For example, use the `GET /name` BOPTEST API endpoint, along with your `testid`, to request the name of your test case and check that it matches the one we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8mdK5JtNI-e_",
        "outputId": "61e28717-0cc9-44f6-99f5-221d2dd4e7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'bestest_hydronic_heat_pump'}\n"
          ]
        }
      ],
      "source": [
        "# Get test case name\n",
        "name = requests.get('{0}/name/{1}'.format(url, testid)).json()['payload']\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUOQXYjlHEj3"
      },
      "source": [
        "With our unique `testid` in-hand and having some practice using the BOPTEST API, we are ready to move on to start using our building emulator. For this tutorial, we are going to explain only how to obtain information about the building using the BOPTEST API before moving to learn BOPTEST-Gym.\n",
        "Note that the test case will timeout after 15 minutes of no requests.  If the test case times out, you can simply select and start a new one by repeating the steps described above.\n",
        "\n",
        "<!-- Note, however, that you can further explore the BOPTEST API by going through [this workshop material](https://github.com/ibpsa/project1-boptest/blob/master/docs/workshops/BS21Workshop_20210831/Introduction_to_the_BOPTEST_framework.ipynb). -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmglIZGFHEj3"
      },
      "source": [
        "## **Obtaining general information about the building** <a name=\"obtainInfo\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ6leLGvRJya"
      },
      "source": [
        "The first thing we want to do is learn about the building and system that we want to control.  All building information can be found under documentation provided for each specific test case on the [Test Cases tab](https://ibpsa.github.io/project1-boptest/testcases/index.html) of the BOPTEST website.\n",
        "\n",
        "The building information includes a description of the building envelope, the HVAC system design, the functioning of the baseline controller, available control inputs and measurement outputs, and available testing scenarios. Understanding how the system works is an important practice for control design, so take as much time as needed to understand the equipment, the points that can be measured, and the points that can be overwritten by your controller.\n",
        "We briefly summarize the `bestest_hydronic_heat_pump` case here for completeness, but it is strongly recommended to have a deeper look into the [documentation](https://ibpsa.github.io/project1-boptest/testcases/ibpsa/testcases_ibpsa_bestest_hydronic_heat_pump/).\n",
        "\n",
        "The building represents a residential dwelling of 192 $m^2$ for a family of 5 members.\n",
        "An air-to-water modulating heat pump of 15 $kW$ nominal heating capacity extracts energy from the ambient air to heat up the floor heating emission system, as shown in the figure below.\n",
        "An evaporator fan blows ambient air through the heat pump evaporator when the heat pump is operating.\n",
        "The floor heating system injects heat into the floor using water as the working fluid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMQcNDl1HEj4"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1-wFA_PnXlJLBV1uwej76_p5PMG8vPhzY\" width=\"500\"/>\n",
        "\n",
        "\n",
        "*Figure: Schematic of HVAC system and control for the `bestest_hydronic_heat_pump` test case.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNLRXp2eHEj4"
      },
      "source": [
        "A baseline controller is embedded in every test case emulator that is meant to be representative of a typical controller for that type of building.  The baseline controller includes local loop control such that supervisory set points may be the focus of a test controller, although many of those local loop control signals are also available for overwriting if a user chooses.  The baseline controller can also be considered an initial benchmark for control performance.\n",
        "\n",
        "In our selected test case, the baseline controller consists of a PI controller with the zone operative temperature as the controlled variable and the heat pump modulation signal for compressor frequency as the control variable, as depicted as C1 in the figure above and shown in the figure below.\n",
        "The control variable is limited between 0 and 1, and it is computed to drive the zone operative temperature towards its set point, which is defined as a function of the occupancy schedule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqVtoDgTHEj4"
      },
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?id=104XbwwPO8hCLNxiUu3fTnNIJ5IjyUFFF\" width=\"400\"/>\n",
        "\n",
        "*Figure: Primary PI controller C1.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryOicW_KHEj5"
      },
      "source": [
        "All other equipment (fan for the heat pump evaporator circuit and floor heating emission system pump) are switched on when the heat pump is working (modulating signal higher than 0) and switched off otherwise.  This is depicted in the figure of the HVAC schematic as controller C2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J5j60bRHEj5"
      },
      "source": [
        "## **Getting control input and measurement points** <a name=\"gettingIOs\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGG7G6VeR4WB"
      },
      "source": [
        "While control input and measurement points are described in the documentation, they are also available to retreive from the BOPTEST API.  This is especially useful to store for later when requesting data for a specific point.\n",
        "\n",
        "Retrieve the control input and measurement outputs using the `GET /inputs` and `GET /measurements` BOPTEST API endpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0IKRxBykJY6u",
        "outputId": "3418cf38-f450-42a2-a26c-504376f3a166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST CASE INPUTS ---------------------------------------------\n",
            "dict_keys(['oveFan_activate', 'oveFan_u', 'oveHeaPumY_activate', 'oveHeaPumY_u', 'ovePum_activate', 'ovePum_u', 'oveTSet_activate', 'oveTSet_u'])\n",
            "TEST CASE MEASUREMENTS ---------------------------------------\n",
            "dict_keys(['reaCO2RooAir_y', 'reaCOP_y', 'reaPFan_y', 'reaPHeaPum_y', 'reaPPumEmi_y', 'reaQFloHea_y', 'reaQHeaPumCon_y', 'reaQHeaPumEva_y', 'reaTRet_y', 'reaTSetCoo_y', 'reaTSetHea_y', 'reaTSup_y', 'reaTZon_y', 'weaSta_reaWeaCeiHei_y', 'weaSta_reaWeaCloTim_y', 'weaSta_reaWeaHDifHor_y', 'weaSta_reaWeaHDirNor_y', 'weaSta_reaWeaHGloHor_y', 'weaSta_reaWeaHHorIR_y', 'weaSta_reaWeaLat_y', 'weaSta_reaWeaLon_y', 'weaSta_reaWeaNOpa_y', 'weaSta_reaWeaNTot_y', 'weaSta_reaWeaPAtm_y', 'weaSta_reaWeaRelHum_y', 'weaSta_reaWeaSolAlt_y', 'weaSta_reaWeaSolDec_y', 'weaSta_reaWeaSolHouAng_y', 'weaSta_reaWeaSolTim_y', 'weaSta_reaWeaSolZen_y', 'weaSta_reaWeaTBlaSky_y', 'weaSta_reaWeaTDewPoi_y', 'weaSta_reaWeaTDryBul_y', 'weaSta_reaWeaTWetBul_y', 'weaSta_reaWeaWinDir_y', 'weaSta_reaWeaWinSpe_y'])\n"
          ]
        }
      ],
      "source": [
        "# Get inputs available\n",
        "inputs = requests.get('{0}/inputs/{1}'.format(url, testid)).json()['payload']\n",
        "print('TEST CASE INPUTS ---------------------------------------------')\n",
        "print(inputs.keys())\n",
        "# Get measurements available\n",
        "print('TEST CASE MEASUREMENTS ---------------------------------------')\n",
        "measurements = requests.get('{0}/measurements/{1}'.format(url, testid)).json()['payload']\n",
        "print(measurements.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4L6Gw6YJU5L"
      },
      "source": [
        "## The naming convention is such that the extension `_y` indicates a measurement point, `_u` indicates the value of an input which can be overwritten by a test controller, and `_activate` indicates the enabling (with value 0 or 1) of a test controller to overwrite the corresponding input value.\n",
        "Hence, `<varname>_u` is enabled for overwriting by the test controller when `<varname>_activate=1`.\n",
        "## `weaSta_` indicates a measurement for a weather point, so that historical weather data can be easily retrieved.\n",
        "\n",
        "Notice that the jsons returned from the `GET /inputs` and `GET /measurements` BOPTEST API endpoints also include a description and unit of each variable, as well as the minimum and maximum value for inputs variables:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23Pedcv-B677"
      },
      "source": [
        "##  üí• **Exercise 2: implement a simple controller in BOPTEST** <a name=\"exerciseBoptest\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABiqj2X0B679"
      },
      "source": [
        "Complete the following code to implement a simple proportional controller in this building test case. You'll need to use the variables `reaTZon_y`, `oveHeaPumY_u`, and `oveHeaPumY_activate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4_FV0w7GB67-"
      },
      "outputs": [],
      "source": [
        "class Controller_Proportional(object):\n",
        "\n",
        "    def __init__(self, TSet=273.15+21, k_p=10.):\n",
        "        '''Constructor.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        TSet : float, optional\n",
        "            Temperature set-point in Kelvin.\n",
        "        k_p : float, optional\n",
        "            Proportional gain.\n",
        "\n",
        "        '''\n",
        "\n",
        "        self.TSet = TSet\n",
        "        self.k_p  = k_p\n",
        "\n",
        "    def compute_control(self, y):\n",
        "\n",
        "      if y['reaTZon_y']<self.TSet:\n",
        "        e = self.TSet - y['reaTZon_y']\n",
        "      else:\n",
        "        e = 0\n",
        "      value = self.k_p*e\n",
        "\n",
        "      u = {'oveHeaPumY_u':value,\n",
        "             'oveHeaPumY_activate': 1}\n",
        "\n",
        "      return u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jqVQoObB68C"
      },
      "source": [
        "Now you run the following cell to test your controller in co-simulation and plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "puIYkMXGB68E",
        "outputId": "586f61a8-3af7-424e-d8e0-19b874661a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------\n",
            "Operative temperature [degC]  = 20.64\n",
            "Simulation time [elapsed days] = 0.96\n",
            "-------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_and_plot_results(testid, start_time, final_time):\n",
        "    '''\n",
        "    Get and plot result key trajectories from the test case.\n",
        "\n",
        "    The key trajectories are zone operative temperature, zone heating and\n",
        "    cooling set points, heat pump electrical power, outside air dry bulb\n",
        "    temperature, and outside direct normal solar irradiation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    testid : str\n",
        "      The testid for the test case.\n",
        "    start_time : float or int\n",
        "      The start time in seconds from the beginning of the year for data.\n",
        "    final_time : float or int\n",
        "      The final time in seconds from the beginning of the year for data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    '''\n",
        "    df_res = pd.DataFrame()\n",
        "    points = ['reaTZon_y', 'reaTSetHea_y', 'reaTSetCoo_y', 'oveHeaPumY_u',\n",
        "              'weaSta_reaWeaTDryBul_y', 'weaSta_reaWeaHDirNor_y']\n",
        "    args = {'point_names':points,\n",
        "            'start_time':start_time,\n",
        "            'final_time':final_time}\n",
        "    res = requests.put('{0}/results/{1}'.format(url, testid),\n",
        "                        data=args).json()['payload']\n",
        "    df_res = pd.DataFrame(data=res)\n",
        "    df_res.set_index('time')\n",
        "    plt.close()\n",
        "    _, axs = plt.subplots(3, sharex=True, figsize=(10,8))\n",
        "    x_time = (start_time+df_res.index)/3600./24.\n",
        "    # Plot operative temperature\n",
        "    axs[0].plot(x_time, df_res['reaTZon_y']    - 273.15, 'darkorange',\n",
        "                linestyle='-', linewidth=0.8, label='$T_z$')\n",
        "    axs[0].plot(x_time, df_res['reaTSetHea_y'] - 273.15, 'gray',\n",
        "                linewidth=0.8, label='Comfort setp.')\n",
        "    axs[0].plot(x_time, df_res['reaTSetCoo_y'] - 273.15, 'gray',\n",
        "                linewidth=0.8, label='_nolegend_')\n",
        "    axs[0].set_yticks(np.arange(15, 31, 5))\n",
        "    axs[0].set_ylabel('Operative\\ntemperature\\n($^\\circ$C)')\n",
        "    axs[0].legend()\n",
        "    # Plot heat pump electrical power\n",
        "    axs[1].plot(x_time, df_res['oveHeaPumY_u'], 'darkorange',\n",
        "                linestyle='-', linewidth=0.8, label='$P_{hp}$')\n",
        "    axs[1].set_ylabel('Heat pump\\nmodulation\\nsignal\\n( - )')\n",
        "    axs[1].legend()\n",
        "    # Plot disturbances\n",
        "    axs[2].plot(x_time, df_res['weaSta_reaWeaTDryBul_y'] - 273.15, 'royalblue',\n",
        "                linestyle='-', linewidth=0.8, label='$T_a$')\n",
        "    axs[2].set_ylabel('Ambient\\ntemperature\\n($^\\circ$C)')\n",
        "    axs[2].set_yticks(np.arange(-5, 16, 5))\n",
        "    axs[2].legend(loc='upper left')\n",
        "    axs[2].set_xlabel('Day of the year')\n",
        "    axt = axs[2].twinx()\n",
        "    axt.plot(x_time, df_res['weaSta_reaWeaHDirNor_y'], 'gold',\n",
        "             linestyle='-', linewidth=0.8, label='$\\dot{Q}_{rad}$')\n",
        "    axt.set_ylabel('Solar\\nirradiation\\n($W$)')\n",
        "    axt.legend(loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "    return None\n",
        "\n",
        "# Initialize scenario\n",
        "y = requests.put('{0}/scenario/{1}'.format(url, testid),\n",
        "                 data={'time_period':'peak_heat_day',\n",
        "                       'electricity_price':'dynamic'}).json()['payload']['time_period']\n",
        "# Get the start time day from the scenario return\n",
        "start_time_days = y['time']/24/3600\n",
        "# Set control step\n",
        "requests.put('{0}/step/{1}'.format(url, testid), data={'step':3600})\n",
        "# Instantiate controller\n",
        "con = Controller_Proportional(TSet=273.15+21, k_p=5.)\n",
        "# Simulation loop\n",
        "from IPython.display import clear_output\n",
        "for _ in range(24):\n",
        "    # Clear the display output at each step\n",
        "    clear_output(wait=True)\n",
        "    # Print the current operative temperature and simulation time\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('Operative temperature [degC]  = {:.2f}'.format(y['reaTZon_y']-273.15))\n",
        "    simulation_time_days = y['time']/3600/24\n",
        "    print('Simulation time [elapsed days] = {:.2f}'.format((simulation_time_days - \\\n",
        "                                                    start_time_days)))\n",
        "    print('-------------------------------------------------------------------')\n",
        "    # Compute control signal\n",
        "    u = con.compute_control(y)\n",
        "    # Advance simulation with control signal\n",
        "    y = requests.post('{0}/advance/{1}'.format(url, testid), data=u).json()['payload']\n",
        "\n",
        "get_and_plot_results(testid, start_time=start_time_days*24*3600, final_time=np.inf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7guJ_I10QOF"
      },
      "source": [
        "Now let's stop the test case since we are not going to use it for a while. We do this to not overwhelm the server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "v5_1Q_H80Z5k",
        "outputId": "0d8cc927-ecb0-4d21-9936-e9054c0f3877"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "requests.put('{0}/stop/{1}'.format(url, testid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUBKXqUWB68N"
      },
      "source": [
        "You will see first a dynamic print with the evolution of the operative temperature and simulation time. Once the cosimulation is finished, a plot is shown with the operative temperature and comfort setpoints, the heat pump modulation signal, the ambient temperature, and the solar irradiation.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHYHM9MjSz_C"
      },
      "source": [
        "# **Part 4: Implementing RL for a building with BOPTEST-Gym** ü§ñ üè† <a name=\"implementingRL\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEC76h9HT7gL"
      },
      "source": [
        "## **What is BOPTEST-Gym?** <a name=\"whatIsBoptestGym\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7A9k7GFUBsK"
      },
      "source": [
        "BOPTEST-Gym is the Gymnasium interface of BOPTEST that helps to train RL agents for the application of building climate control.\n",
        "The BOPTEST-Gym interface accomodates the BOPTEST API to have BOPTEST building emulators as environments that follow the Gymnasium standard.\n",
        "Therefore, the BOPTEST-Gym interface facilitates the development of RL agents as it allows interacting with the BOPTEST building emulators with a standard that is very well known by the machine learning community. Or even better, it allows us to directly use existing RL agents that have been developed following this standard, like those from the [Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/) repository.\n",
        "\n",
        "You can find more information about BOPTEST-Gym in [this paper](https://publications.ibpsa.org/conference/paper/?id=bs2021_30380), but here we summarize the main points you should know:\n",
        "- BOPTEST-Gym enables the interaction of RL agents with a set of physics-based and highly **detailed building models** to assess RL for the application of building climate control.\n",
        "- All **hyperparameters** of the environment are initialized when the environment is instantiated. A particularly relevant hyperparameter is `testcase`, a string specifying the BOPTEST emulator of choice. This string selects the building model from the [menu of BOPTEST building emulators](https://ibpsa.github.io/project1-boptest/testcases/index.html).\n",
        "- The **state** of any building emulator environment can have a *time* component e.g. a weekly schedule, a *measurement* component with a subset (or all) measurements available in the building, and an *exogenous* component including disturbances of any kind of boundary condition data to the building such as electricity prices, ambient temperature, or temperature set-points.\n",
        "- The **action** space is defined based on any subset (or all) inputs available to the emulator. These can be either building set-points, like zone\n",
        "operative temperature set-points, or lower level actuator signals, such as heat\n",
        "pump modulating signal or a pump stage.\n",
        "- The **`reset()`** method is called at the beginning of every episode to return the environment to a logical initial state.\n",
        "- The **`step()`** method is called every time step to take the action computed by the RL agent, overwrite the building inputs with the vector of action values and advance the building simulation model during one time step period. BOPTEST-Gym also has wrappers for discretization of the state and action spaces. This functionality comes in handy when training RL agents.\n",
        "- A default **reward** function is implemented in the `compute_reward` method of the BOPTEST-Gym environment that can be overwritten. It is convenient to use the BOPTEST `/kpis` API to obtain the KPI values at the present time for defining custom reward functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvMgiRhLX2i8"
      },
      "source": [
        "## **Starting up a BOPTEST-Gym environment** <a name=\"startingUpBoptestGym\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcrOX1Z_UvTY"
      },
      "source": [
        "BOPTEST-Gym uses RL algorithms from the [Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/) repository to exemplify and test its functionality. Therefore, we need to install stable-baselines3.\n",
        " <!-- PyTorch, the neural network framework that Stable Baselines uses to configure policies and value functions.  -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jpZk6qJKTuYl",
        "outputId": "c7c63708-5704-4d95-804a-f0a5cca6c091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3==2.0.0 in /usr/local/lib/python3.12/dist-packages (2.0.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: gymnasium==0.28.1 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.0.0) (0.28.1)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.0.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.0.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.0.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.0.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.0.0) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.0.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.0.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.0.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.0.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.0.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.0.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11->stable-baselines3==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11->stable-baselines3==2.0.0) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3==2.0.0 numpy==1.26.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ljXzd7W4R0H"
      },
      "source": [
        "Now that we have all package dependencies, let's clone the BOPTEST-Gym repository. We are going to clone the `boptest-gym-service` branch which works in the same way as the `master` branch but allows us to directly use the web-based version of BOPTEST that is readily available such that we do not have to deploy the building test case Docker containers locally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2adMIlJu_ZZ8",
        "outputId": "03625cee-accd-4e72-bc7e-51ad953fb10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'boptestGymService'...\n",
            "remote: Enumerating objects: 4333, done.\u001b[K\n",
            "remote: Counting objects: 100% (547/547), done.\u001b[K\n",
            "remote: Compressing objects: 100% (210/210), done.\u001b[K\n",
            "remote: Total 4333 (delta 396), reused 437 (delta 315), pack-reused 3786 (from 2)\u001b[K\n",
            "Receiving objects: 100% (4333/4333), 50.26 MiB | 11.41 MiB/s, done.\n",
            "Resolving deltas: 100% (2327/2327), done.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  !rm -rf boptestGymService\n",
        "except:\n",
        "  pass\n",
        "!git clone -b boptest-gym-service https://github.com/ibpsa/project1-boptest-gym.git boptestGymService"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs9guwYo5w50"
      },
      "source": [
        "Now we move our working directory to our recently cloned repository, import the `BoptestGymEnv` class, and instantiate our first BOPTEST-Gym environment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mZsXUZIQ5iIj",
        "outputId": "307e3692-4666-4af2-b6c4-83a113fe9894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/content/boptestGymService/examples/test_and_plot.py:122: SyntaxWarning: invalid escape sequence '\\c'\n",
            "  axs[0].set_ylabel('Operative\\ntemperature\\n($^\\circ$C)')\n",
            "/content/boptestGymService/examples/test_and_plot.py:131: SyntaxWarning: invalid escape sequence '\\c'\n",
            "  axs[3].set_ylabel('Ambient\\ntemperature\\n($^\\circ$C)')\n",
            "/content/boptestGymService/examples/test_and_plot.py:135: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  axt.plot(x_time, df['weaSta_reaWeaHDirNor_y'], color='gold', linestyle='-', linewidth=1, label='$\\dot{Q}_rad$')\n",
            "/content/boptestGymService/examples/test_and_plot.py:141: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  axs[3].plot([],[], color='gold',        linestyle='-', linewidth=1, label='$\\dot{Q}_{rad}$')\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0,'boptestGymService')\n",
        "from boptestGymEnv import BoptestGymEnv\n",
        "\n",
        "# Instantiate environment\n",
        "env = BoptestGymEnv(url                   = url,\n",
        "                    testcase              = 'bestest_hydronic_heat_pump',\n",
        "                    actions               = ['oveHeaPumY_u'],\n",
        "                    observations          = {'reaTZon_y':(280.,310.)},\n",
        "                    random_start_time     = False,\n",
        "                    start_time            = 31*24*3600,\n",
        "                    max_episode_length    = 24*3600,\n",
        "                    warmup_period         = 24*3600,\n",
        "                    step_period           = 3600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XVI61rnU4QZ"
      },
      "source": [
        "You have connected to a BOPTEST building emulator and wrapped it around a Gym environment. Let's examine more in detail the arguments that you have used:\n",
        "- `url`: the domain where your test case lives. In this case it is the url to BOPTEST-service, but it could be your localhost if you decide to spin a test case in your machine using Docker.\n",
        "- `testcase`: The string identifier of the testcase.\n",
        "- `actions`: List of strings indicating the action space.\n",
        "- `observations`: Dictionary mapping observation keys to a tuple with the lower and upper bound of each observation. These bounds define the typical operational range for discretization and normalization purposes. Observation keys must belong either to the set of measurements or to the set of forecasting variables of the BOPTEST test case.\n",
        "- `max_episode_lenght`: Maximum duration of each episode in seconds.\n",
        "- `random_start_time`: Set to True if desired to use a random start time for each episode. That is typically usefull when training an RL agent to run several episodes with different boundary condition data. In our case, we set it to False and specify the start time of the episode.\n",
        "- `start_time`: start time of the episode. It is specified in seconds from the beginning of the year. To be used in combination with `random_start_time=False`.  \n",
        "- `warmup_period`: Desired simulation period to initialize each episode, in seconds. In our case, we simulate the testcase for one day right before the beginning of the episode.\n",
        "- `step_period`: The period of each control step, in seconds. In this case is set to one hour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZcNOH0SYEiR"
      },
      "source": [
        "Now you can interact with the building emulator following the Gym standard. Everytime you use one of the methods of your environment, BOPTEST-Gym will send the associated commands through the BOPTEST API that you have learned above as to provide the desired functionality. A schematic of this process is shown in the figure below. This figure illustrates the typical steps that take place when training an agent and the mapping between the BOPTEST-Gym interface and the BOPTEST API. It is important to note that a state can be returned not only with current measurements, but also with boundary condition forecast or regressive values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcnaIJvhYDa5"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1057swXWvL11gyCqLCrUg9ooS1crD-1fp\" width=\"700\"/>\n",
        "\n",
        "*Figure: Sequence diagram for training an agent withthe BOPTEST-Gym environment.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZVIz69qXyCZ"
      },
      "source": [
        "## **Interacting with a BOPTEST-Gym environment** <a name=\"interactingWithBoptestGym\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA9_wVo8bMxr"
      },
      "source": [
        "Let's see what we can do with our building Gym environment. Recall that the first step is using the `reset` method to simulate the building right before the episode start time a time period specified in `warmup_period`. This will bring the building to a reasonable initial state and the environment will return an observation `obs` which, in our case, it is comprised of only the zone operative temperature (`reaTZon_y`). This temperature is in Kelvins, so we convert it to degrees Celsius."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z4n5GsjXV08x",
        "outputId": "6a8bff5e-1604-4bfc-9cf6-7d25d8f2ccc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zone temperature: 21.37 degC\n",
            "Episode starting day: 31.0 (from beginning of the year)\n"
          ]
        }
      ],
      "source": [
        "obs, _ = env.reset()\n",
        "print('Zone temperature: {:.2f} degC'.format(obs[0]-273.15))\n",
        "print('Episode starting day: {:.1f} (from beginning of the year)'.format(env.start_time/24/3600))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeJyBvLYqC11"
      },
      "source": [
        "üìå **Note: About initialization**\n",
        "\n",
        "The initial state in the emulator consists of all states after simulation during the warmup period without any external input from an external controller. This particular emulator has 63 continuous time states comprising temperatures of walls, floor, roof, water, etc. During the warmup period, the baseline controller embedded in the emulator is used. After initialization the baseline controller will also work at any time unless some of the control variables are intentionally overwritten by an external controller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i9VfDrdYJ0e"
      },
      "source": [
        "We can inspect the observation and action space of any environment as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a_PC0YAEYR5U",
        "outputId": "0da82d18-aff4-41dc-e365-d88471ba2fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space of the building environment:\n",
            "Box(280.0, 310.0, (1,), float32)\n",
            "Action space of the building environment:\n",
            "Box(0.0, 1.0, (1,), float32)\n"
          ]
        }
      ],
      "source": [
        "print('Observation space of the building environment:')\n",
        "print(env.observation_space)\n",
        "print('Action space of the building environment:')\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBVCnncbePIQ"
      },
      "source": [
        "So this environment has a Box (continuous and bounded) observation space which is the indoor building temperature. The operational range of this variable goes from $280$ $K$ to $310$ $K$. That is, from ~$7$ $¬∞C$ to $37$ $¬∞C$. On the other hand, the action space is a continuous variable that goes from $0$ to $1$. The latter variable represents the heat pump compressor frequency with $0$ meaning no heating, and $1$ meaning the heat pump working at full capacity.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkx5Os6-Yltb"
      },
      "source": [
        "But actually, the BOPTEST-Gym environment can be directly printed to show a lot of useful information to control the building:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FGzL_ZskfoyO",
        "outputId": "ae9913d0-27c6-40e3-80dd-42123c1b1a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================\n",
            "BOPTEST CASE INFORMATION\n",
            "========================\n",
            "\n",
            "Test case name\n",
            "--------------\n",
            "{'name': 'bestest_hydronic_heat_pump'}\n",
            "\n",
            "All measurement variables\n",
            "-------------------------\n",
            "{'reaCO2RooAir_y': {'Description': 'CO2 concentration in the zone',\n",
            "                    'Maximum': None,\n",
            "                    'Minimum': None,\n",
            "                    'Unit': 'ppm'},\n",
            " 'reaCOP_y': {'Description': 'Heat pump COP',\n",
            "              'Maximum': None,\n",
            "              'Minimum': None,\n",
            "              'Unit': '1'},\n",
            " 'reaPFan_y': {'Description': 'Electrical power of the heat pump evaporator '\n",
            "                              'fan',\n",
            "               'Maximum': None,\n",
            "               'Minimum': None,\n",
            "               'Unit': 'W'},\n",
            " 'reaPHeaPum_y': {'Description': 'Heat pump electrical power',\n",
            "                  'Maximum': None,\n",
            "                  'Minimum': None,\n",
            "                  'Unit': 'W'},\n",
            " 'reaPPumEmi_y': {'Description': 'Emission circuit pump electrical power',\n",
            "                  'Maximum': None,\n",
            "                  'Minimum': None,\n",
            "                  'Unit': 'W'},\n",
            " 'reaQFloHea_y': {'Description': 'Floor heating thermal power released to the '\n",
            "                                 'zone',\n",
            "                  'Maximum': None,\n",
            "                  'Minimum': None,\n",
            "                  'Unit': 'W'},\n",
            " 'reaQHeaPumCon_y': {'Description': 'Heat pump thermal power exchanged in the '\n",
            "                                    'condenser',\n",
            "                     'Maximum': None,\n",
            "                     'Minimum': None,\n",
            "                     'Unit': 'W'},\n",
            " 'reaQHeaPumEva_y': {'Description': 'Heat pump thermal power exchanged in the '\n",
            "                                    'evaporator',\n",
            "                     'Maximum': None,\n",
            "                     'Minimum': None,\n",
            "                     'Unit': 'W'},\n",
            " 'reaTRet_y': {'Description': 'Return water temperature from radiant floor',\n",
            "               'Maximum': None,\n",
            "               'Minimum': None,\n",
            "               'Unit': 'K'},\n",
            " 'reaTSetCoo_y': {'Description': 'Zone operative temperature setpoint for '\n",
            "                                 'cooling',\n",
            "                  'Maximum': None,\n",
            "                  'Minimum': None,\n",
            "                  'Unit': 'K'},\n",
            " 'reaTSetHea_y': {'Description': 'Zone operative temperature setpoint for '\n",
            "                                 'heating',\n",
            "                  'Maximum': None,\n",
            "                  'Minimum': None,\n",
            "                  'Unit': 'K'},\n",
            " 'reaTSup_y': {'Description': 'Supply water temperature to radiant floor',\n",
            "               'Maximum': None,\n",
            "               'Minimum': None,\n",
            "               'Unit': 'K'},\n",
            " 'reaTZon_y': {'Description': 'Zone operative temperature',\n",
            "               'Maximum': None,\n",
            "               'Minimum': None,\n",
            "               'Unit': 'K'},\n",
            " 'weaSta_reaWeaCeiHei_y': {'Description': 'Cloud cover ceiling height '\n",
            "                                          'measurement',\n",
            "                           'Maximum': None,\n",
            "                           'Minimum': None,\n",
            "                           'Unit': 'm'},\n",
            " 'weaSta_reaWeaCloTim_y': {'Description': 'Day number with units of seconds',\n",
            "                           'Maximum': None,\n",
            "                           'Minimum': None,\n",
            "                           'Unit': 's'},\n",
            " 'weaSta_reaWeaHDifHor_y': {'Description': 'Horizontal diffuse solar radiation '\n",
            "                                           'measurement',\n",
            "                            'Maximum': None,\n",
            "                            'Minimum': None,\n",
            "                            'Unit': 'W/m2'},\n",
            " 'weaSta_reaWeaHDirNor_y': {'Description': 'Direct normal radiation '\n",
            "                                           'measurement',\n",
            "                            'Maximum': None,\n",
            "                            'Minimum': None,\n",
            "                            'Unit': 'W/m2'},\n",
            " 'weaSta_reaWeaHGloHor_y': {'Description': 'Global horizontal solar '\n",
            "                                           'irradiation measurement',\n",
            "                            'Maximum': None,\n",
            "                            'Minimum': None,\n",
            "                            'Unit': 'W/m2'},\n",
            " 'weaSta_reaWeaHHorIR_y': {'Description': 'Horizontal infrared irradiation '\n",
            "                                          'measurement',\n",
            "                           'Maximum': None,\n",
            "                           'Minimum': None,\n",
            "                           'Unit': 'W/m2'},\n",
            " 'weaSta_reaWeaLat_y': {'Description': 'Latitude of the location',\n",
            "                        'Maximum': None,\n",
            "                        'Minimum': None,\n",
            "                        'Unit': 'rad'},\n",
            " 'weaSta_reaWeaLon_y': {'Description': 'Longitude of the location',\n",
            "                        'Maximum': None,\n",
            "                        'Minimum': None,\n",
            "                        'Unit': 'rad'},\n",
            " 'weaSta_reaWeaNOpa_y': {'Description': 'Opaque sky cover measurement',\n",
            "                         'Maximum': None,\n",
            "                         'Minimum': None,\n",
            "                         'Unit': '1'},\n",
            " 'weaSta_reaWeaNTot_y': {'Description': 'Sky cover measurement',\n",
            "                         'Maximum': None,\n",
            "                         'Minimum': None,\n",
            "                         'Unit': '1'},\n",
            " 'weaSta_reaWeaPAtm_y': {'Description': 'Atmospheric pressure measurement',\n",
            "                         'Maximum': None,\n",
            "                         'Minimum': None,\n",
            "                         'Unit': 'Pa'},\n",
            " 'weaSta_reaWeaRelHum_y': {'Description': 'Outside relative humidity '\n",
            "                                          'measurement',\n",
            "                           'Maximum': None,\n",
            "                           'Minimum': None,\n",
            "                           'Unit': '1'},\n",
            " 'weaSta_reaWeaSolAlt_y': {'Description': 'Solar altitude angle measurement',\n",
            "                           'Maximum': None,\n",
            "                           'Minimum': None,\n",
            "                           'Unit': 'rad'},\n",
            " 'weaSta_reaWeaSolDec_y': {'Description': 'Solar declination angle measurement',\n",
            "                           'Maximum': None,\n",
            "                           'Minimum': None,\n",
            "                           'Unit': 'rad'},\n",
            " 'weaSta_reaWeaSolHouAng_y': {'Description': 'Solar hour angle measurement',\n",
            "                              'Maximum': None,\n",
            "                              'Minimum': None,\n",
            "                              'Unit': 'rad'},\n",
            " 'weaSta_reaWeaSolTim_y': {'Description': 'Solar time',\n",
            "                           'Maximum': None,\n",
            "                           'Minimum': None,\n",
            "                           'Unit': 's'},\n",
            " 'weaSta_reaWeaSolZen_y': {'Description': 'Solar zenith angle measurement',\n",
            "                           'Maximum': None,\n",
            "                           'Minimum': None,\n",
            "                           'Unit': 'rad'},\n",
            " 'weaSta_reaWeaTBlaSky_y': {'Description': 'Black-body sky temperature '\n",
            "                                           'measurement',\n",
            "                            'Maximum': None,\n",
            "                            'Minimum': None,\n",
            "                            'Unit': 'K'},\n",
            " 'weaSta_reaWeaTDewPoi_y': {'Description': 'Dew point temperature measurement',\n",
            "                            'Maximum': None,\n",
            "                            'Minimum': None,\n",
            "                            'Unit': 'K'},\n",
            " 'weaSta_reaWeaTDryBul_y': {'Description': 'Outside drybulb temperature '\n",
            "                                           'measurement',\n",
            "                            'Maximum': None,\n",
            "                            'Minimum': None,\n",
            "                            'Unit': 'K'},\n",
            " 'weaSta_reaWeaTWetBul_y': {'Description': 'Wet bulb temperature measurement',\n",
            "                            'Maximum': None,\n",
            "                            'Minimum': None,\n",
            "                            'Unit': 'K'},\n",
            " 'weaSta_reaWeaWinDir_y': {'Description': 'Wind direction measurement',\n",
            "                           'Maximum': None,\n",
            "                           'Minimum': None,\n",
            "                           'Unit': 'rad'},\n",
            " 'weaSta_reaWeaWinSpe_y': {'Description': 'Wind speed measurement',\n",
            "                           'Maximum': None,\n",
            "                           'Minimum': None,\n",
            "                           'Unit': 'm/s'}}\n",
            "\n",
            "All forecasting variables\n",
            "-------------------------\n",
            "['PriceElectricPowerConstant',\n",
            " 'PriceElectricPowerDynamic',\n",
            " 'PriceElectricPowerHighlyDynamic',\n",
            " 'pAtm',\n",
            " 'lon',\n",
            " 'lat',\n",
            " 'TDewPoi',\n",
            " 'HDirNor',\n",
            " 'HDifHor',\n",
            " 'HGloHor',\n",
            " 'relHum',\n",
            " 'nTot',\n",
            " 'ceiHei',\n",
            " 'winSpe',\n",
            " 'winDir',\n",
            " 'nOpa',\n",
            " 'TBlaSky',\n",
            " 'HHorIR',\n",
            " 'cloTim',\n",
            " 'solZen',\n",
            " 'solAlt',\n",
            " 'solDec',\n",
            " 'solTim',\n",
            " 'solHouAng',\n",
            " 'TDryBul',\n",
            " 'TWetBul',\n",
            " 'Occupancy[1]',\n",
            " 'LowerSetp[1]',\n",
            " 'InternalGainsLat[1]',\n",
            " 'UpperSetp[1]',\n",
            " 'InternalGainsCon[1]',\n",
            " 'InternalGainsRad[1]',\n",
            " 'UpperCO2[1]',\n",
            " 'EmissionsElectricPower']\n",
            "\n",
            "All input variables\n",
            "-------------------\n",
            "{'oveFan_activate': {'Description': 'Activation for Integer signal to control '\n",
            "                                    'the heat pump evaporator fan either on or '\n",
            "                                    'off',\n",
            "                     'Maximum': None,\n",
            "                     'Minimum': None,\n",
            "                     'Unit': None},\n",
            " 'oveFan_u': {'Description': 'Integer signal to control the heat pump '\n",
            "                             'evaporator fan either on or off',\n",
            "              'Maximum': 1,\n",
            "              'Minimum': 0,\n",
            "              'Unit': '1'},\n",
            " 'oveHeaPumY_activate': {'Description': 'Activation for Heat pump modulating '\n",
            "                                        'signal for compressor speed between 0 '\n",
            "                                        '(not working) and 1 (working at '\n",
            "                                        'maximum capacity)',\n",
            "                         'Maximum': None,\n",
            "                         'Minimum': None,\n",
            "                         'Unit': None},\n",
            " 'oveHeaPumY_u': {'Description': 'Heat pump modulating signal for compressor '\n",
            "                                 'speed between 0 (not working) and 1 (working '\n",
            "                                 'at maximum capacity)',\n",
            "                  'Maximum': 1,\n",
            "                  'Minimum': 0,\n",
            "                  'Unit': '1'},\n",
            " 'ovePum_activate': {'Description': 'Activation for Integer signal to control '\n",
            "                                    'the emission circuit pump either on or '\n",
            "                                    'off',\n",
            "                     'Maximum': None,\n",
            "                     'Minimum': None,\n",
            "                     'Unit': None},\n",
            " 'ovePum_u': {'Description': 'Integer signal to control the emission circuit '\n",
            "                             'pump either on or off',\n",
            "              'Maximum': 1,\n",
            "              'Minimum': 0,\n",
            "              'Unit': '1'},\n",
            " 'oveTSet_activate': {'Description': 'Activation for Zone operative '\n",
            "                                     'temperature setpoint',\n",
            "                      'Maximum': None,\n",
            "                      'Minimum': None,\n",
            "                      'Unit': None},\n",
            " 'oveTSet_u': {'Description': 'Zone operative temperature setpoint',\n",
            "               'Maximum': 308.15,\n",
            "               'Minimum': 278.15,\n",
            "               'Unit': 'K'}}\n",
            "\n",
            "Default simulation step (seconds)\n",
            "---------------------------------\n",
            "3600\n",
            "\n",
            "Default scenario\n",
            "----------------\n",
            "{'electricity_price': 'constant',\n",
            " 'seed': None,\n",
            " 'solar_uncertainty': None,\n",
            " 'temperature_uncertainty': None}\n",
            "\n",
            "Test case scenario\n",
            "------------------\n",
            "{'electricity_price': 'constant'}\n",
            "\n",
            "===========================\n",
            "GYM ENVIRONMENT INFORMATION\n",
            "===========================\n",
            "\n",
            "Observation space\n",
            "-----------------\n",
            "Box(280.0, 310.0, (1,), float32)\n",
            "\n",
            "Action space\n",
            "------------\n",
            "Box(0.0, 1.0, (1,), float32)\n",
            "\n",
            "Is a regressive environment\n",
            "---------------------------\n",
            "False\n",
            "\n",
            "Is a predictive environment\n",
            "---------------------------\n",
            "False\n",
            "\n",
            "Regressive period (seconds)\n",
            "---------------------------\n",
            "None\n",
            "\n",
            "Predictive period (seconds)\n",
            "---------------------------\n",
            "None\n",
            "\n",
            "Measurement variables used in observation space\n",
            "-----------------------------------------------\n",
            "['reaTZon_y']\n",
            "\n",
            "Predictive variables used in observation space\n",
            "----------------------------------------------\n",
            "[]\n",
            "\n",
            "Sampling time (seconds)\n",
            "-----------------------\n",
            "3600\n",
            "\n",
            "Random start time\n",
            "-----------------\n",
            "False\n",
            "\n",
            "Excluding periods (seconds from the beginning of the year)\n",
            "----------------------------------------------------------\n",
            "None\n",
            "\n",
            "Warmup period for each episode (seconds)\n",
            "----------------------------------------\n",
            "86400\n",
            "\n",
            "Maximum episode length (seconds)\n",
            "--------------------------------\n",
            "86400\n",
            "\n",
            "Environment reward function (source code)\n",
            "-----------------------------------------\n",
            "('    def get_reward(self):\\n'\n",
            " \"        '''\\n\"\n",
            " \"        Compute the reward of last state-action-state' tuple. The \\n\"\n",
            " '        reward is implemented as the negated increase in the objective\\n'\n",
            " '        integrand function. In turn, this objective integrand function \\n'\n",
            " '        is calculated as the sum of the total operational cost plus\\n'\n",
            " '        the weighted discomfort. \\n'\n",
            " '        \\n'\n",
            " '        Returns\\n'\n",
            " '        -------\\n'\n",
            " '        Reward: float\\n'\n",
            " \"            Reward of last state-action-state' tuple\\n\"\n",
            " '        \\n'\n",
            " '        Notes\\n'\n",
            " '        -----\\n'\n",
            " '        This method is just a default method to compute reward. It can be \\n'\n",
            " '        overridden by defining a child from this class with\\n'\n",
            " '        this same method name, i.e. `get_reward`. If a custom reward \\n'\n",
            " '        is defined, it is strongly recommended to derive it using the KPIs\\n'\n",
            " '        as returned from the BOPTEST framework, as it is done in this \\n'\n",
            " '        default `get_reward` method. This ensures that all variables \\n'\n",
            " '        that may contribute to any KPI are properly accounted and \\n'\n",
            " '        integrated. \\n'\n",
            " '        \\n'\n",
            " \"        '''\\n\"\n",
            " '        \\n'\n",
            " '        # Define a relative weight for the discomfort \\n'\n",
            " '        w = 1\\n'\n",
            " '        \\n'\n",
            " '        # Compute BOPTEST core kpis\\n'\n",
            " '        kpis = '\n",
            " \"requests.get('{0}/kpi/{1}'.format(self.url,self.testid)).json()['payload']\\n\"\n",
            " '        \\n'\n",
            " '        # Calculate objective integrand function at this point\\n'\n",
            " \"        objective_integrand = kpis['cost_tot'] + w*kpis['tdis_tot']\\n\"\n",
            " '        \\n'\n",
            " '        # Compute reward\\n'\n",
            " '        reward = -(objective_integrand - self.objective_integrand)\\n'\n",
            " '        \\n'\n",
            " '        self.objective_integrand = objective_integrand\\n'\n",
            " '        \\n'\n",
            " '        return reward\\n')\n",
            "\n",
            "Environment hierarchy\n",
            "---------------------\n",
            "(<class 'boptestGymEnv.BoptestGymEnv'>,\n",
            " <class 'gymnasium.core.Env'>,\n",
            " <class 'typing.Generic'>,\n",
            " <class 'object'>)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KYf1BksgfQj"
      },
      "source": [
        "Note that this descriptive summary provides information not only about the Gym environment but also all information about the original BOPTEST test case. This may be useful, for example, if we want to extend our observation space or if we want to change our control action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ_y22lrg1cM"
      },
      "source": [
        "BOPTEST-Gym comes along with other functionality that may be useful when training RL agents, like the capacity to discretize and normalize observation and action spaces. For instance, we are dealing now with continuous action environment meaning that the agent could decide to take any action between 0 and 1. However, it is probably helpful to the agent to decide on just whether the heating needs to be turned on (action=1) or off (action=0). For that, we can wrap our environment around a discretization wrapper with only one action bin (one bin has two extremes).  The concept of wrappers is very powerful in Gym environments. With them, we are capable to customize observation, action, step function, etc. of an environment. No matter how many wrappers are applied, `env.unwrapped` always gives back the internal original environment object. Let's see how it works with BOPTEST-Gym:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zIqfeNwgh9VK",
        "outputId": "6ae58f4d-15d3-4e84-f1a4-9ce89cb6f507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action space of the wrapped agent:\n",
            "Discrete(2)\n",
            "Action space of the original agent:\n",
            "Box(0.0, 1.0, (1,), float32)\n"
          ]
        }
      ],
      "source": [
        "from boptestGymEnv import DiscretizedActionWrapper\n",
        "env = DiscretizedActionWrapper(env,n_bins_act=1)\n",
        "print('Action space of the wrapped agent:')\n",
        "print(env.action_space)\n",
        "print('Action space of the original agent:')\n",
        "print(env.unwrapped.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0tBy1ODB68v"
      },
      "source": [
        "##  üí• **Exercise 3: implement a simple controller in BOPTEST-Gym** <a name=\"exerciseBoptestGym\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghlx_zaf282q"
      },
      "source": [
        "Another thing that we can do is to interact with the building environment for one episode of experience (one day). This is similar to what we did with the Cartpole example, but this time we are going to run just one episode and use a hysteresis controller that will turn on the heating the temperature is below a predefined temperature setpoint, and turn it off when the temperature goes above the setpoint. Try to implement such controller in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MrO0o7hNf5pB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "class SimpleController(object):\n",
        "    '''Simple controller for this emulator.\n",
        "\n",
        "    '''\n",
        "    def __init__(self, TSet=22+273.15):\n",
        "        self.TSet = TSet\n",
        "\n",
        "    def predict(self, obs):\n",
        "      if obs[0]<self.TSet:\n",
        "        action = np.asarray(1) # Turn on heating\n",
        "      else:\n",
        "        action = np.asarray(0) # No heating needed\n",
        "      return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGgb8vKYn1ce"
      },
      "source": [
        "And now we implement the controller in a simulation loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DVOu1AK2Avz1",
        "outputId": "ad24164d-4133-4b9f-a43a-4e570a70f4db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------\n",
            "Operative temperature [degC]  = 22.05\n",
            "Action                [ - ]   = 0\n",
            "-------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = SimpleController(TSet=22+273.15)\n",
        "\n",
        "done = False\n",
        "obs, _ = env.reset()\n",
        "\n",
        "from IPython.display import clear_output\n",
        "while not done:\n",
        "  # Clear the display output at each step\n",
        "  clear_output(wait=True)\n",
        "  # Compute control signal\n",
        "  action = model.predict(obs)\n",
        "  # Print the current operative temperature and decided action\n",
        "  print('-------------------------------------------------------------------')\n",
        "  print('Operative temperature [degC]  = {:.2f}'.format(obs[0]-273.15))\n",
        "  print('Action                [ - ]   = {:.0f}'.format(action))\n",
        "  print('-------------------------------------------------------------------')\n",
        "  # Implement action\n",
        "  obs,reward,terminated,truncated,info = env.step(action) # send the action to the environment\n",
        "  done = (terminated or truncated)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "model = SimpleController(TSet=22+273.15)\n",
        "\n",
        "done = False\n",
        "obs, _ = env.reset()\n",
        "\n",
        "# --- storage for plotting\n",
        "temps = []\n",
        "actions = []\n",
        "times  = []\n",
        "\n",
        "t = 0\n",
        "while not done:\n",
        "    # Compute control signal\n",
        "    action = model.predict(obs)\n",
        "\n",
        "    # Save values\n",
        "    temps.append(obs[0] - 273.15)   # convert K ‚Üí ¬∞C\n",
        "    actions.append(action)\n",
        "    times.append(t)\n",
        "\n",
        "    # Step the environment\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    done = (terminated or truncated)\n",
        "\n",
        "    # --- live plotting\n",
        "    clear_output(wait=True)\n",
        "    fig, ax1 = plt.subplots(figsize=(7,4))\n",
        "\n",
        "    ax1.set_xlabel(\"Step\")\n",
        "    ax1.set_ylabel(\"Temperature [¬∞C]\", color=\"tab:red\")\n",
        "    ax1.plot(times, temps, color=\"tab:red\", label=\"Room Temp\")\n",
        "    ax1.axhline(22, color=\"tab:orange\", linestyle=\"--\", label=\"Setpoint\")\n",
        "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
        "\n",
        "    ax2 = ax1.twinx()  # second axis for action\n",
        "    ax2.set_ylabel(\"Action\", color=\"tab:blue\")\n",
        "    ax2.step(times, actions, color=\"tab:blue\", where=\"post\", label=\"Action\")\n",
        "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('Operative temperature [degC]  = {:.2f}'.format(temps[-1]))\n",
        "    print('Action                        = {:.0f}'.format(action))\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "    t += 1\n"
      ],
      "metadata": {
        "id": "OZrQTV2AIRCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFN2U0ZYDFV4"
      },
      "source": [
        "Note how the controller decides to turn on heating (`action=1`) when indoor temperature is below the setpoint, and it turns it off when the temperature is above the setpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwCVNv94AxJP"
      },
      "source": [
        "‚ö†Ô∏è **Important note:** ‚ö†Ô∏è The controller is instantiated as `model` because RL agents typically use models (e.g. any general function approximator like neural networks to represent their policies). The choice of the instance name is arbitrary, but `model` has been historically accepted in different RL frameworks like Stable Baselines. We could have chosen any other name, but we use `model` to start getting familiar with this convention. The `predict` method is used to estimate the action to be taken according to the RL agent's model.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlYHloCEB68-"
      },
      "source": [
        "You should see a dynamic print with the evolution of the operative temperature and the action taken by your controller agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKBjnzycaCS4"
      },
      "source": [
        "## **Developing a basic RL algorithm** <a name=\"developingRlAlgo\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAIt_IfivAHN"
      },
      "source": [
        "In this section we are going to develop a very simple RL agent based on the very well known *q-learning* algorithm. Although simple, this exercise will help us understand the main concepts of RL and how this machine learning technique can be helpful to mitigate climate change by enhancing building's operational efficiency. Recall that our objective is to develop an RL agent that can decide on the best action to take in each situation (each state) just from interactions with the environment (the building). Imagine we are at time $k$ in a certain state $\\pmb{s}$ and take an action $\\pmb{a}$. In return, we obtain a reward $r'$ the next time step and end up in a state $\\pmb{s}'$ :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxO3gVx3vqrL"
      },
      "source": [
        "![](https://drive.google.com/file/d/1XVbDEiHT2fWIGtnPLE0uphC2hV5XubKc/view?usp=sharing)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1C_ARg1Ycn6KatsEpFYWURLPMaSwDp57v\" width=\"300\"/>\n",
        "\n",
        "*Figure: The backup diagram. Edited version from the book of Richard S. Sutton and Andrew G. Barto* **[6]**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XflFYx7lylyw"
      },
      "source": [
        "In *q-learning* we aim to derive an *action-value function*, the q-function. The q-function indicates what is the **long-term** value of taking an action $a$ from a certain state $s$. With this information we not only have an estimation of the value of each state, but we can also decide to take the next action $\\pmb{a}'$ that leads to the highest value from the next state $\\pmb{s}'$. This principle relies on the so-called *Bellman optimality equation* that is presented below:\n",
        "\n",
        "\\begin{align}\n",
        "  q(\\pmb{s},\\pmb{a}) = r' + \\gamma \\max_{\\pmb{a}'} q(\\pmb{s}',\\pmb{a}')\n",
        "\\end{align}\n",
        "\n",
        "This equation states that the total expected cummulative return of taking action $\\pmb{a}$ from state $\\pmb{s}$ equals the immediate reward $r'$ plus the maximum achievable reward that we can obtain from the following state $\\pmb{s}'$. Note that the q-function estimates the **TOTAL EXPECTED CUMULATIVE RETURN** of taking action $\\pmb{a}$ from state $\\pmb{s}$ (not just the immediate reward). So given the q-function we can know straight-away what is the best action to take for each state $\\pmb{s}$. You can imagine a q-function with one-dimensional state and action spaces as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBpa3qjuysK-"
      },
      "source": [
        "<a name=\"qFunctionConcept\"></a>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1k1j__DXF3AWTxMN_pH42LTuajkiYzf-H\" width=\"300\"/>\n",
        "\n",
        "*Figure: Example of how a q-function may look like for the case with one-dimensional state and action spaces. Note that, given the q-function, we can pick the action $a$ that leads to the highest expected cumulative reward $q_*$ from state $s$.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db8AVf3GoCz9"
      },
      "source": [
        "Powerful, right? now the question remains how to derive the q-function üòÖ.\n",
        "\n",
        "The q-function is inferred iteratively using the reward received by the agent each control step and bootstrapping with the Bellman optimality equation presented above. The sum of the immediate reward and the next-state q-function estimate is called the target. We use this target to recursively update the q-function at a learning rate $\\alpha$. The difference between the target and our current q-function estimate is called *temporal difference*. In summary, the q-learning method consists of recursively updating the q-function using the following formula:\n",
        "\n",
        "\\begin{align}\n",
        "  q(\\pmb{s},\\pmb{a}) = q(\\pmb{s},\\pmb{a}) + \\alpha [ \\underbrace{\\underbrace{r' + \\gamma \\max_{\\pmb{a}'} q(\\pmb{s}',\\pmb{a}')}_\\text{target} - q(\\pmb{s},\\pmb{a})}_\\text{temporal difference}]\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjBqNfXd_pY2"
      },
      "source": [
        "So in summary, the agent observes the reward once it has taken an action from a state. It has to explore the rewards from different state-action pairs and update its q-function as it goes.\n",
        "\n",
        "In our example we are going to use tabular state and action spaces to expedite learning and to easily store and visualize the q-function. Note, however, that we could use general function approximators like neural networks to configure the q-function.\n",
        "\n",
        "üìå **Note: The exploration-exploitation dilema** ‚öñÔ∏è\n",
        "\n",
        "RL always faces the so-called exploration-exploitation dilema. That is, how much of what we have learned we should exploit and how much we should explore to find even better solutions? In our case, we implement an *Epsilon-greedy* approach to balance exploration and exploitation of the RL agent. That is, the agent sometimes picks a random action (exploration), and sometimes picks an \"intelligent\" action (exploitation). The frequency at which the agent picks a random action is determined by *Epsilon* (`eps`) and it follows a linearly decaying schedule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ7Um2UtLHk4"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1Tw2jNRdRe9KKa2i7z3gdu4dcKnqXLIxG\" width=\"400\"/>\n",
        "\n",
        "*Figure: The epsilon-greedy strategy for balancing exploration and exploitation.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aOHW96nLqOZ"
      },
      "source": [
        "Our `Q_Learning_Agent` consists of only three methods:\n",
        "\n",
        "- `__init__` ‚û°Ô∏è The constructor.\n",
        "- `predict` ‚û°Ô∏è Method to decide on an action given an observation.\n",
        "- `learn` ‚û°Ô∏è Method for learning with the q-learning method explained above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "9U81QUVcUfoW"
      },
      "outputs": [],
      "source": [
        "class Q_Learning_Agent(object):\n",
        "\n",
        "  def __init__(self, env, eps_min=0.01, eps_decay=0.01, alpha=0.05, gamma=0.9):\n",
        "    '''Constructor of a q-learning agent. Assumes discrete state and action spaces.\n",
        "\n",
        "    '''\n",
        "    self.env       = env\n",
        "    self.eps_min   = eps_min\n",
        "    self.eps_decay = eps_decay\n",
        "    self.alpha     = alpha\n",
        "    self.gamma     = gamma\n",
        "\n",
        "    # Initialize epsilon\n",
        "    self.eps       = 1.0\n",
        "\n",
        "    # Initialize q-function as a null function\n",
        "    self.q = np.zeros((env.observation_space.n,\n",
        "                       env.action_space.n))\n",
        "\n",
        "  def predict(self, obs, deterministic=True):\n",
        "    '''Method to select an action with an epsilon-greedy policy.\n",
        "\n",
        "    '''\n",
        "    if deterministic:\n",
        "      # Use q-function to decide action\n",
        "      return np.argmax(self.q[obs])\n",
        "    else:\n",
        "      if self.eps > self.eps_min:\n",
        "        # Linearly decreasing schedule\n",
        "        self.eps -= self.eps_decay\n",
        "      if np.random.random() < self.eps:\n",
        "        # Explore with random action\n",
        "        return np.random.choice([a for a in range(env.action_space.n)])\n",
        "      else:\n",
        "        # Exploit the information of our q-function\n",
        "        return np.argmax(self.q[obs])\n",
        "\n",
        "  def learn(self, total_episodes=10):\n",
        "    '''Learn from a number of interactions with the environment.\n",
        "\n",
        "    '''\n",
        "    for i in range(total_episodes):\n",
        "      # Initialize enviornment\n",
        "      done = False\n",
        "      obs, _  = env.reset()\n",
        "      # Print episode number and starting day from beginning of the year:\n",
        "      print('-------------------------------------------------------------------')\n",
        "      print('Episode number: {0}, starting day: {1:.1f} ' \\\n",
        "            '(from beginning of the year)'.format(i+1, env.unwrapped.start_time/24/3600))\n",
        "\n",
        "      while not done:\n",
        "        # Get action with epsilon-greedy policy and simulate\n",
        "        act                   = self.predict(obs, deterministic=False)\n",
        "        nxt_obs, rew, terminated, truncated, _ = env.step(act)\n",
        "        done = (terminated or truncated)\n",
        "        # Compute temporal difference target and error to udpate q-function\n",
        "        td_target         = rew + self.gamma*np.max(self.q[nxt_obs])\n",
        "        td_error          = td_target - self.q[obs][act]\n",
        "        self.q[obs][act] += self.alpha*td_error\n",
        "        # Make our next observation the current observation\n",
        "        obs = nxt_obs\n",
        "      # Print the q-function after every episode to show progress\n",
        "      print('q(s,a) = ')\n",
        "      print(self.q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWPbW8WKaQET"
      },
      "source": [
        "## **Testing our RL algorithm in BOPTEST-Gym** <a name=\"testingRlAlgo\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV8bk8x75C_0"
      },
      "source": [
        "Now that we have a RL agent ready, let's test it in BOPTEST-Gym! We are going to exploit the features of BOPTEST-Gym to:\n",
        "\n",
        "- Define a custom reward function of the enviornment.\n",
        "- Instantiate the environment and define its state and action spaces.\n",
        "- Train our RL agent.\n",
        "\n",
        "<!-- - Exclude periods that are not interesting for learning (we will only keep the Winter period).\n",
        "- Discretize the state and action spaces of the environment.  -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy1TpSGEPxYr"
      },
      "source": [
        "### Define a custom reward function of the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9jwn5BQQCyj"
      },
      "source": [
        "The definition of the reward function is **KEY**üóù since it is what drives the learning of an agent.\n",
        "The `BoptestGymEnv` Class allows to override its `get_reward` method that is called every control step as to freely define any reward function of choice.\n",
        "\n",
        "In our example, the goal is to implement a RL agent to identify the actions that keep comfort inside the building, and we should encode our reward function accordingly. We could implement this function by integrating the temperature deviations out of the comfort range. However, this approach is error-prone. We typically want to directly use signals from the environment to define the reward, preferrably those that are directly related to the function we want to optimize so that we make sure we strive for the ground truth optimum. In BOPTEST we use the `GET /kpis` API to obtain the so-called core KPIs at the present time, which are:\n",
        "\n",
        "\n",
        "*   **Thermal discomfort**:  reported with units of [$K \\, h/zone$], defines the cumulative deviation of zone temperatures from upper and lower comfort limits that are predefined within the test case FMU for each zone, averaged over all zones.  Air temperature is used for air-based systems and operative temperature is used for radiant systems.\n",
        "*   **Indoor Air Quality (IAQ) Discomfort**: reported with units of [$ppm \\, h/zone$], defines the extent that the CO$_2$ concentration levels in zones exceed  bounds of the acceptable concentration level, which are predefined within the test case FMU for each zone, averaged over all zones.\n",
        "*   **Energy Use**: reported with units of [$kWh/m^2$], defines the HVAC energy usage.\n",
        "*   **Cost**: reported with units of [USD/$m^2$] or [EUR/$m^2$], defines the operational cost associated with the HVAC energy usage.\n",
        "*   **Emissions**: reported with units of  [$kg \\, CO_2/m^2$], defines the CO$_2$ emissions from the HVAC energy usage.\n",
        "*   **Computational time ratio**: defines the average ratio between the controller computation time and the test simulation control step. The controller computation time is measured as the time between two emulator advances.\n",
        "\n",
        "The time series graph below shows how thermal discomfort and energy use are computed by the BOPTEST `GET /kpis` API call.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQagxsvtr3ow"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1EceXX2LlgsDTGuqjnii8hRONHMRX--Wi\" width=\"500\"/>\n",
        "\n",
        "*Figure: Integration of thermal discomfort (top) and energy use (bottom). In BOPTEST, the `GET /kpis` API can directly return these values every control step. Note that the integration step is significantly smaller than the control step.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esAUAwHdr8y1"
      },
      "source": [
        "The core KPIs are normally calculated at the end of the simulation to assess the controller performance, although they can be computed at any time.  The warmup period is not taken into account for the calculation of the KPIs. See below how we define the `get_reward` method using the `GET /kpi`. Every control step we check whether there has been a discomfort increment. If there is not discomfort increment, we reward our agent with $1$, otherwise we return a $0$ (no reward). Clipping the reward is a good practice to accelerate learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "hTcc3XiVP-A6"
      },
      "outputs": [],
      "source": [
        "# Redefine reward function\n",
        "class BoptestGymEnvCustomReward(BoptestGymEnv):\n",
        "    '''Define a custom reward for this building\n",
        "\n",
        "    '''\n",
        "    def get_reward(self):\n",
        "        '''Custom reward function. To expedite learning, we use a clipped reward\n",
        "        function that has a value of 1 when there is no increase in discomfort\n",
        "        and 0 otherwise. We use the BOPTEST `GET /kpis` API call to compute the\n",
        "        total cummulative discomfort from the beginning of the episode. Note\n",
        "        that this is the true value that BOPTEST uses when evaluating\n",
        "        controllers.\n",
        "\n",
        "        '''\n",
        "        # Compute BOPTEST core kpis\n",
        "        kpis = requests.get('{0}/kpi/{1}'.format(self.url, self.testid)).json()['payload']\n",
        "        # Calculate objective integrand function as the total discomfort\n",
        "        objective_integrand = kpis['tdis_tot']\n",
        "        # Give reward if there is not immediate increment in discomfort\n",
        "        if objective_integrand == self.objective_integrand:\n",
        "          reward=1\n",
        "        else:\n",
        "          reward=0\n",
        "        # Record current objective integrand for next evaluation\n",
        "        self.objective_integrand = objective_integrand\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hpd_svcOhDy"
      },
      "source": [
        "### Instantiate the environment and define its state and action spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xszlVIQtOkiz"
      },
      "source": [
        "Similarly to our `SimpleController` example, now we are going to use an agent that observes only the current indoor temperature and decides whether to turn heating on or off. However, instead of hard-coding such logic, we are going to use our very own implementation of the `Q_Learning_Agent` to see if it can learn how to do that.\n",
        "For this, we are going to let our RL agent interact with the building for some episodes of experience.\n",
        "Since we are now going to run several episodes for training, we want to stop our previous environment and start one that randomly initializes our building emulator throughout the year.\n",
        "This allows to train our agent when using different boundary condition data in our building environment. We are also going to exclude the Spring, Summer, and Fall periods for training since we are only focused on learning the heating behavior.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "24fsDMTv8tSF",
        "outputId": "320d775d-068e-40ea-ae14-b2f38b8f6f78"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#env.close() #added close instead of stop because stop was not wokrinh\n",
        "env.stop()\n",
        "import random\n",
        "\n",
        "# Seed for random starting times of episodes\n",
        "seed = 123456\n",
        "random.seed(seed)\n",
        "# Seed for random exploration and epsilon-greedy schedule\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Winter period goes from December 21 (day 355) to March 20 (day 79)\n",
        "excluding_periods = [(79*24*3600, 355*24*3600)]\n",
        "# Temperature setpoints\n",
        "lower_setp = 21 + 273.15\n",
        "upper_setp = 24 + 273.15\n",
        "# Instantiate environment\n",
        "env = BoptestGymEnvCustomReward(url                   = url,\n",
        "                                testcase              = 'bestest_hydronic_heat_pump',\n",
        "                                actions               = ['oveHeaPumY_u'],\n",
        "                                observations          = {'reaTZon_y':(lower_setp,upper_setp)},\n",
        "                                random_start_time     = True,\n",
        "                                excluding_periods     = excluding_periods,\n",
        "                                max_episode_length    = 2*24*3600,\n",
        "                                warmup_period         = 24*3600,\n",
        "                                step_period           = 3600,\n",
        "                                render_episodes       = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU8aoMvV9AdE"
      },
      "source": [
        "We have set the zone temperature as the only observation of the environment state. We have also set the lower and upper bounds of this variable to be $21$ and $24 ¬∞C$, respectively, which are the bounds of the comfort range during occupied periods. These bounds can be used by the environment for normalization or discretization purposes. In fact, we are going to discretize both the action and observation spaces to expedite learning. We decide to set only one bin for the action space (two possible actions: heating on or off). We split the observation space in three bins with the outer bounds of the comfort range as bins of the observation space (`outs_are_bins=True`). That is, the observation space is defined by $[-‚àû,21,24,+‚àû]$ as shown on the left hand side of the figure below. Note that only the middle bin is always comfortable whereas the other bins may lead to discomfort. If we had set `outs_are_bins=False` we would have had all our bins within the comfort range. The latter would give the agent a notion of what is the temperature within the comfort range (close to the lower bound, middle, or close to the upper bound), but it would raise an error if the temperature is out of the range.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "uCUZKrOMOIEN"
      },
      "outputs": [],
      "source": [
        "from boptestGymEnv import DiscretizedObservationWrapper\n",
        "env = DiscretizedActionWrapper(env, n_bins_act=1)\n",
        "env = DiscretizedObservationWrapper(env, n_bins_obs=3, outs_are_bins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab6WP3zLEvnb"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ZyGf_Bl8irJ6-ifCSNKgpnkz9j2BRs1e\" width=\"800\"/>\n",
        "\n",
        "*Figure: Possibilities for the discretization of the state space.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTvGxERwOOI6"
      },
      "source": [
        "### Train our RL agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc3XqYSDOuGq"
      },
      "source": [
        "The only missing step is to let our RL agent learn by rolling out episodes of experience with the environment. We use the previously defined `learn` method for this. Note that, since we set `render_episodes=True`, we will be seeing a plot with relevant variables after each episode is finished. This is helpful to check if the agent is learning as expected from early stages. If the agent is not showing any sign of life we can prematurely stop the learning process to use new learning settings while saving some valuable time and computational cost.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "jtOpX5y_RTsV",
        "outputId": "ce89d64e-bd91-4eaa-a191-b3e38f10308d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'float' object cannot be interpreted as an integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2647114975.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_Learning_Agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1811275781.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_episodes)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0;31m# Initialize enviornment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0;31m# Print episode number and starting day from beginning of the year:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------------------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    460\u001b[0m     ) -> tuple[WrapperObsType, dict[str, Any]]:\n\u001b[1;32m    461\u001b[0m         \u001b[0;34m\"\"\"Modifies the :attr:`env` after calling :meth:`reset`, returning a modified observation using :meth:`self.observation`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    412\u001b[0m     ) -> tuple[WrapperObsType, dict[str, Any]]:\n\u001b[1;32m    413\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/boptestGymService/boptestGymEnv.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;31m# Assign random start_time if it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_start_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_start_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# Initialize the building simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/boptestGymService/boptestGymEnv.py\u001b[0m in \u001b[0;36mfind_start_time\u001b[0;34m()\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             '''\n\u001b[0;32m--> 468\u001b[0;31m             start_time = random.randint(0+self.bgn_year_margin, \n\u001b[0m\u001b[1;32m    469\u001b[0m                                         3.1536e+7-self.end_year_margin)\n\u001b[1;32m    470\u001b[0m             \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_episode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/random.py\u001b[0m in \u001b[0;36mrandint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/random.py\u001b[0m in \u001b[0;36mrandrange\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# Stop argument supplied.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mistop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mistop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mistart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mistep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
          ]
        }
      ],
      "source": [
        "model = Q_Learning_Agent(env, eps_min=0.01, eps_decay=0.001, alpha=0.1, gamma=0.9)\n",
        "model.learn(total_episodes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qblijq9WVobQ"
      },
      "source": [
        "Since our environment has been defined with one-dimensional state and action spaces, we can plot the q-function after training as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_u32nxmzSDm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acts   = ['a=0','a=1']\n",
        "stas   = ['T<21', '21<T<24', 'T>24']\n",
        "colors = ['b',    'g',       'r']\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.set_xlabel('actions',           labelpad=6,  fontsize=12)\n",
        "ax.set_ylabel('states',            labelpad=10, fontsize=12)\n",
        "ax.set_zlabel('$\\mathbf{q(s,a)}$', labelpad=0,  fontsize=15)\n",
        "plt.xticks(ticks=range(len(acts)), labels=acts)\n",
        "plt.yticks(ticks=range(len(stas)), labels=stas)\n",
        "\n",
        "for i, s in enumerate(stas):\n",
        "  x = np.arange(len(acts))\n",
        "  h = model.q[i,:]\n",
        "\n",
        "  # Set color\n",
        "  color = [colors[i]]*len(acts)\n",
        "\n",
        "  # Plot the 3D bar graph\n",
        "  ax.bar(x, h, zs=i, zdir='y', color=color, alpha=0.8)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0AIl-HeVyqs"
      },
      "source": [
        "The q-function was conceptually introduced at the beginning of Section [*Developing a basic RL algorithm*](#developingRlAlgo). A more formal definition is provided in [*Annex I*](#theoryRlFormal).\n",
        "We observe that the state with the highest value is the one in the middle (green bars üü¢üëå, `21<T<24`) because this is the one that keeps comfort. In that state, there is no significant difference between turning heating on or off. On the contrary, when temperature is below the comfort bound (blue bars üîµü•∂, `T<21`), there is more value on taking action `a=1`, so there is a preference for the agent to turn heating on. On the other hand, when temperature is above the comfort bound (red bars üî¥üî•, `T>24`), there is more value on `a=0`, so there is a preference for the agent to turn heating off.\n",
        "\n",
        "Sometimes it is useful to know what is the value of being on a specific state, independently of the action to be taken. This is represented by the so-called state-value function, which relates to the action-value function as follows:\n",
        "\n",
        "\\begin{align}\n",
        "  v(\\pmb{s}) = \\max_{\\pmb{a}} q(\\pmb{s},\\pmb{a})\n",
        "\\end{align}\n",
        "\n",
        "At this point we can easily compute and plot the value function for our case:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urJOkjSNoa-h"
      },
      "outputs": [],
      "source": [
        "# Compute the state-value function\n",
        "v = np.amax(model.q, axis=1)\n",
        "\n",
        "# Plot state-value function\n",
        "fig = plt.figure()\n",
        "\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlabel('states', labelpad=10, fontsize=12)\n",
        "ax.set_ylabel('$\\mathbf{v(s)}$', labelpad=0,  fontsize=15)\n",
        "plt.xticks(ticks=range(len(stas)), labels=stas)\n",
        "x = np.arange(len(stas))\n",
        "ax.bar(x, v, color=colors, alpha=0.8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clFn8dd7obRI"
      },
      "source": [
        "Notice that we have trained our agent following an off-policy method: the actions were driven by a policy different than that one that our agent would follow. This is because the agent was using an epsilon-greedy policy to explore more rewarding actions. If we conclude we are happy with the learned policy, we can test it by setting `deterministic=True` with the `predict` method. For example, let's test our learned agent for the first day of February:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuYEBf9nsmH6"
      },
      "outputs": [],
      "source": [
        "env.stop()\n",
        "env = BoptestGymEnvCustomReward(url                   = url,\n",
        "                                testcase              = 'bestest_hydronic_heat_pump',\n",
        "                                actions               = ['oveHeaPumY_u'],\n",
        "                                observations          = {'reaTZon_y':(lower_setp,upper_setp)},\n",
        "                                random_start_time     = False,\n",
        "                                start_time            = 31*24*3600,\n",
        "                                max_episode_length    = 24*3600,\n",
        "                                warmup_period         = 24*3600,\n",
        "                                step_period           = 3600)\n",
        "env = DiscretizedActionWrapper(env, n_bins_act=1)\n",
        "env = DiscretizedObservationWrapper(env, n_bins_obs=3, outs_are_bins=True)\n",
        "\n",
        "done = False\n",
        "obs, _ = env.reset()\n",
        "\n",
        "from IPython.display import clear_output\n",
        "while not done:\n",
        "  # Clear the display output at each step\n",
        "  clear_output(wait=True)\n",
        "  # Compute control signal\n",
        "  action = model.predict(obs, deterministic=True)\n",
        "  # Print the current operative temperature and decided action\n",
        "  print('-------------------------------------------------------------------')\n",
        "  print('State  [Bin #]  = {:.0f}'.format(obs))\n",
        "  print('Action [ - ]    = {:.0f}'.format(action))\n",
        "  print('-------------------------------------------------------------------')\n",
        "  # Implement action\n",
        "  obs,reward,terminated,truncated,info = env.step(action) # send the action to the environment\n",
        "  done = (terminated or truncated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLBD3joyxe9Z"
      },
      "source": [
        "Now there is no randomness involved. The agent exploits its policy by ALWAYS picking action `a=1` when `s=0` because it has learned that that is the action with the highest value in that state.\n",
        "\n",
        "We can now evaluate our learned policy by calculating the core KPIs with BOPTEST:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLzZaaNzyeZv"
      },
      "outputs": [],
      "source": [
        "env.get_kpis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbDAlStV2Tvx"
      },
      "source": [
        "This prepares the ground for different RL configurations to be evaluated and compared between each other and to other types of controls like classical rule based controllers or more advanced model predictive control. Recall that there are specific [scenario periods for each test case in BOPTEST](https://github.com/ibpsa/project1-boptest/tree/master/testcases#test-cases) that are set for these comparisons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m59XicauB69z"
      },
      "source": [
        "##  üí• **Exercise 4: expedite learning in BOPTEST-Gym** <a name=\"exerciseBoptestGymExpedite\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBL289bfsmcJ"
      },
      "source": [
        "Could you think of measures to accelerate learning? Could you think of measures to improve performance? Are these two correlated?\n",
        "\n",
        "<!-- If the agent never receives a reward when the temperature is out of the comfort bounds (states 0 üîµ and 2 üî¥), why is the q-function not 0 for those states? -->\n",
        "\n",
        "Tune and retrain your RL agent above to observe how different factors affect learning. Quick hacks that you can try out are:\n",
        "\n",
        "- Redefine the `BoptestGymEnvCustomReward` e.g. to return `-1` instead of `0` if there is immediate increment in discomfort.\n",
        "- Play around with the hyperparameters of the Q-learning agent, that is: `eps_min`, `eps_decay`, `alpha` and `gamma`.\n",
        "\n",
        "To improve performance you may need increase the degrees of freedom of the agent and to provide it with a more complete observation vector.\n",
        "\n",
        "- Modify `DiscretizedActionWrapper` to use 10 action bins.\n",
        "- Augment the observation space by using variables that could help optimizing the control policy like the ambient temperature, solar irradiation, the internal gains, the electricity price, or the comfort setpoints.\n",
        "\n",
        "Define an environment with these last two suggestions.\n",
        "Once created, pring the observation and action space.\n",
        "The new environment is probably something we cannot easily learn with our simple Q-learning algorithm because of the high dimension of our new observation space.\n",
        "However, there is available a DQN algorithm in Stable-Baselines that is more sophisticated and incorporates tricks to learn complex environments.\n",
        "Import the DQN algorithm from Stable-Baselines and have it learn the new environment for 10 interaction steps. Learning a meaningful control policy would take way longer. This is just tow learn how to initiate learning with this new algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKqvn5yb_mqJ"
      },
      "source": [
        "### **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1sdwYm5b66G"
      },
      "source": [
        "The previously stylished example had a very limited representation of the state space. It was useful to illustrate how we can configure and train a RL agent without needing too many interactions with the environment (our building). However, using RL for solving this environment may feel like overkilling the problem. Our `SimpleController` was already enough to decide when to turn on heating based on indoor temperature readings. You should note, however, that you have developed a general agent capable of learning from any environment and the potential to infer way more complex relationships between environment observations and actions. Examples of what this RL agent could infer for building control are the following:\n",
        "- Dynamic energy pricing\n",
        "- A heating schedule based on user inputs.\n",
        "- A heating curve based on ambient temperature.\n",
        "- The variable heat pump COP based on condenser, evaporator, and ambient temperature reaadings.\n",
        "\n",
        "We could for example extend our reward function as to minimize the building energy use or the greenhouse gas emissions while keeping comfort.\n",
        "And all this can be inferred without the need of a model that requires domain knowledge. On the downside, learning more complex dynamics from higher dymensional observation spaces requires more training data. This means that more interactions with the environment (the building) are required, which sometimes are unavailable. For this reason, sample-efficiency is key in RL and there exist several tricks to expedite learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3ZF29MChF4F"
      },
      "source": [
        "To finalize, we are going to instantiate a more complete building environment by extending the observation space with the time of the week as well as information about the ambient temeprature, solar irradiation, internal gains, electricity pricing, or temperature setpoints. With BOPTEST-Gym we can also establish a predictive and a regressive period that include predictions of the boundary condition data and past observations of the measured data, respectively.\n",
        "\n",
        "Because of its high dimensional state-action space, an agent will probably require many more interactions to solve this environment. Luckily, there are readily available state-of-the-art RL algorithms that use the learning principle you have learned above while implement all sort of tricks to expedite and stabilize learning. For example, we can access the advanced Deep Q-Network (DQN) algorithm from Stable-Baselines3 to learn this more complex environment. We set here our agent to learn for `10` steps to show how this learning process would be initiated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdx3qCDvhFSX"
      },
      "outputs": [],
      "source": [
        "# env.stop()\n",
        "\n",
        "from boptestGymEnv import BoptestGymEnv, NormalizedObservationWrapper, DiscretizedActionWrapper\n",
        "from stable_baselines3 import DQN\n",
        "\n",
        "# url for the BOPTEST service\n",
        "url = 'https://api.boptest.net'\n",
        "\n",
        "# Part 1\n",
        "# Decide the state-action space of your test case\n",
        "env = BoptestGymEnv(\n",
        "        url                  = url,\n",
        "        testcase             = 'bestest_hydronic_heat_pump',\n",
        "        actions              = ['oveHeaPumY_u'],\n",
        "        observations         = {### YOUR CODE HERE ###},\n",
        "        predictive_period    = ### YOUR CODE HERE ###,\n",
        "        regressive_period    = ### YOUR CODE HERE ###,\n",
        "        random_start_time    = True,\n",
        "        max_episode_length   = 24*3600,\n",
        "        warmup_period        = 24*3600,\n",
        "        step_period          = 3600)\n",
        "\n",
        "# Part 2\n",
        "# Normalize observations and discretize action space\n",
        "env = ### YOUR CODE HERE ###\n",
        "\n",
        "# Part 3\n",
        "# Instantiate an RL agent\n",
        "model = ### YOUR CODE HERE ###\n",
        "\n",
        "# Main training loop\n",
        "model.learn(total_timesteps=10)\n",
        "\n",
        "# Loop for one episode of experience (one day as set in max_episode_length)\n",
        "done = False\n",
        "obs, _ = env.reset()\n",
        "while not done:\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  obs,reward,terminated,truncated,info = env.step(action)\n",
        "  done = (terminated or truncated)\n",
        "\n",
        "# Obtain KPIs for evaluation\n",
        "env.get_kpis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vudahvPN_ZaA"
      },
      "source": [
        "Learning for 10 interaction steps is clearly not enough and leads to poor performance. This new environment has a way higher dimensional state-action space than the ones we treated before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSg90XCe-26Q"
      },
      "outputs": [],
      "source": [
        "print('Observation space of the building environment (dimension):')\n",
        "print(env.observation_space.shape)\n",
        "print('Action space of the building environment:')\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOs93H9X_ZaA"
      },
      "source": [
        "Solving an environment of these dimensions requires millions of steps or other tricks to accelerate learning. Could you think of any?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDJHCuQ2NFN6"
      },
      "outputs": [],
      "source": [
        "env.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRtUYIRCB7DA"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHd7hv-_B7DC"
      },
      "outputs": [],
      "source": [
        "# Part 1\n",
        "env = BoptestGymEnv(\n",
        "    url                  = url,\n",
        "    testcase             = 'bestest_hydronic_heat_pump',\n",
        "    actions              = ['oveHeaPumY_u'],\n",
        "    observations         = {'time':(0,604800),\n",
        "                            'reaTZon_y':(280.,310.),\n",
        "                            'TDryBul':(265,303),\n",
        "                            'HDirNor':(0,862),\n",
        "                            'InternalGainsRad[1]':(0,219),\n",
        "                            'PriceElectricPowerHighlyDynamic':(-0.4,0.4),\n",
        "                            'LowerSetp[1]':(280.,310.),\n",
        "                            'UpperSetp[1]':(280.,310.)},\n",
        "    predictive_period    = 24*3600,\n",
        "    regressive_period    = 6*3600,\n",
        "    random_start_time    = True,\n",
        "    max_episode_length   = 24*3600,\n",
        "    warmup_period        = 24*3600,\n",
        "    step_period          = 3600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCUxNCodB7DH"
      },
      "outputs": [],
      "source": [
        "# Part 2\n",
        "env = NormalizedObservationWrapper(env)\n",
        "env = DiscretizedActionWrapper(env,n_bins_act=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91nyEgz7B7DJ"
      },
      "outputs": [],
      "source": [
        "# Part 3\n",
        "model = DQN('MlpPolicy', env, verbose=1, gamma=0.99,\n",
        "            learning_rate=5e-4, batch_size=24, seed=123456,\n",
        "            buffer_size=365*24, learning_starts=24, train_freq=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OUdwgoHB7DK"
      },
      "source": [
        "You should see prints with KPIs as well as the dimensions of the observation and action spaces that you have defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-_f2qRTB0Nw"
      },
      "source": [
        "# **Further resources** <a name=\"furtherResources\"></a> üìö"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p54OK_TGrtfp"
      },
      "source": [
        "- For RL, check out the resources page from Stable-Baselines 3 [here](https://stable-baselines3.readthedocs.io/en/master/guide/rl.html) and the [open access book of Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/the-book-2nd.html)\n",
        "- For BOPTEST, check out the websites of the [BOPTEST framework](https://ibpsa.github.io/project1-boptest/), its [GitHub repository](https://ibpsa.github.io/project1-boptest/), and its overarching project: [IBPSA Project 1](https://ibpsa.github.io/project1/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jblq_C7CHQHj"
      },
      "source": [
        "# **Feedback** <a name=\"feedbackForm\"></a> üí¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ9lmUndHLMq"
      },
      "source": [
        "Please help us improve by filling out [this form](https://forms.gle/JdprK6tgxQtwvhFV8). It'll only take a couple of minutes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNB5MoRmOWc9"
      },
      "source": [
        "# **Annex I: Formal Reinforcement Learning theory** <a name=\"theoryRlFormal\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-ZId2TdCngy"
      },
      "source": [
        "In RL we aim to derive an optimal control policy from the direct interaction of an agent (the RL algorithm) and an environment (the process to be optimized).\n",
        "A policy is a mapping from environment states to actions that the agent \"decides\" to take.\n",
        "This control method is based on the principle of dynamic programming. Unlike\n",
        "classical dynamic programming, RL does not assume the existence of a perfect\n",
        "system model and uses function approximations to build a policy from samples\n",
        "of historical data. Hence, the agent performs empirical learning and decides on\n",
        "actions to drive the environment towards favorable trajectories according to a reward function that the environment delivers every control step.\n",
        "\n",
        "The process of the RL agent interacting with the environment is a sequential decision-making problem formalized as a **Markov Decission Process (MDP)**. A diagram summarizing the RL approach is shown in the following figure:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=10EbdxTgq4PLl2oQp_quLHrht59pDMob6\" width=\"500\"/>\n",
        "\n",
        "*Figure: Diagram of the RL approach. The RL agent decides an action. After the action is implemented, the environment returns the new state $\\pmb{S}_{k+1}$ and associated reward $R_{k+1}$.*\n",
        "\n",
        "In an MDP, the agent and the environment interact during a sequence of discrete-time steps indexed here as $k=0,1,2,...,K$, with $K$ being the terminal sample that could be $K=\\infty$.\n",
        "Every time step $k$ the agent receives a representation of the environment named state: $\\pmb{S}_k \\in \\pmb{\\mathcal{S}}$, where $\\pmb{\\mathcal{S}}$ is the state space.\n",
        "Note that the agent's observation of the state-space may or may not fully characterize the environment state.\n",
        "In the latter case where the agent can only see a partial observation of the environment's state-space, we refer to **partially observable Markov decision processes (POMDPs)**.\n",
        "\n",
        "Upon receiving the state representation, the agent computes its control logic and in turn sends back to the environment a control action $\\pmb{A}_k \\in \\pmb{\\mathcal{A}}$, where $\\pmb{A}_k$ is the most appropriate action chosen from the action space $\\pmb{\\mathcal{A}}$.\n",
        "One time step later, the agent observes a new state from the environment $\\pmb{S}_{k+1}$ along with a scalar value indicating its reward $R_{k+1} \\in \\mathcal{R} \\subset{\\mathbb{R}}$. Notice that the reward $R_{k+1}$ is an indicator of the agent's performance when taking action $\\pmb{A}_k$ from state $\\pmb{S}_k$.\n",
        "\n",
        "The environment $\\mathcal{E}_{\\pmb{f}}$ is governed by the natural laws of the system dynamics $\\pmb{f}$ and it is defined by $\\mathcal{E}_{\\pmb{f}}:\\pmb{\\mathcal{S}}\\times \\pmb{\\mathcal{A}} \\rightarrow \\pmb{\\mathcal{S}}\\times \\mathcal{R}$.\n",
        "The goal of RL is to infer an **optimal control policy** $\\pi_{*}:\\pmb{\\mathcal{S}} \\rightarrow \\pmb{\\mathcal{A}}$ that maximizes the **expected cumulative return** $G$ when the agent acts according to it.\n",
        "The cumulative return is defined as some function of the rewards sequence, and a typical definition is to discount the rewards with a **discount factor** $\\gamma \\in [0,1]$ as shown in the following equation:\n",
        "\n",
        "\\begin{align}\n",
        "  G_k = R_{k+1} + \\gamma R_{k+2} + \\gamma^2 R_{k+3} + ... = \\sum_{i=0}^\\infty \\gamma^i R_{k+i+1}\n",
        "\\end{align}\n",
        "\n",
        "The **action-value function** $q(\\pmb{S},\\pmb{A})$ estimates the expected return when being in a specific state $\\pmb{S}$ and taking an action $\\pmb{A}$.\n",
        "The **state-value function** $v(\\pmb{S})$ directly estimates the expected return for being in state $\\pmb{S}$.\n",
        "Frequently, the policy and value functions are approximated by **function approximations** to cope with high-dimensional state-action spaces.\n",
        "Examples of commonly used regressors are neural networks or randomized trees.\n",
        "\n",
        "<!-- Both value functions can be related as shown in the equation below.\n",
        "\\begin{equation}\n",
        "    v(\\pmb{S}) = \\max_{\\pmb{a}\\in\\pmb{\\mathcal{A}}} q(\\pmb{S}, \\pmb{a})\n",
        "\\end{equation} -->\n",
        "\n",
        "A **trajectory** of an MDP is defined as a sequence of states, actions and rewards.\n",
        "Most of the RL algorithms learn from finite trajectories of experience called **episodes**.\n",
        "Sometimes, the trajectories are broken down into tuples of the form $(\\pmb{s}_k,\\pmb{a}_k,r_k,\\pmb{s}_{k+1})$ and stored in a **replay memory** $\\pmb{\\mathcal{D}}$.\n",
        "Using a replay memory allows to serve the historical data in random batches of tuples to preserve as much as possible the independent and identically distributed assumption that is typically taken to parametrize policies and value functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RIO07aKaQHG"
      },
      "source": [
        "# **References** <a name=\"references\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQM-3Ra5BYM7"
      },
      "source": [
        "\n",
        "- **[1]** *Blum, D., Arroyo, J., Huang, S., Drgona, J., Jorissen, F., Taxt Walnum, H., Yan, C., Benne, K., Vrabie, D., Wetter, M., and Helsen,\n",
        "L. Building Optimization Testing Framework (BOPTEST) for Simulation-\n",
        "Based Benchmarking of Control Strategies in Buildings. Journal of Building\n",
        "Performance Simulation 14, 5 (2021), 586‚Äì610. https://doi.org/10.1080/19401493.2021.1986574*\n",
        "\n",
        "- **[2]** *Arroyo, J., Manna, C., Spiessens, F., and Helsen, L. An OpenAI-Gym\n",
        "environment for the Building Optimization Testing (BOPTEST) framework.\n",
        "In Proceedings of the 17th IBPSA Conference (Bruges, Belgium, September 2021) [https://doi.org/10.26868/25222708.2021.30380](https://www.conftool.pro/bs2021/index.php/30380_Arroyo_Javier.pdf?page=downloadPaper&filename=30380_Arroyo_Javier.pdf&form_id=30380)*\n",
        "\n",
        "- **[3]** *Drgona, J., Arroyo, J., Cupeiro Figueroa, I., Blum, D., Arendt, K., Kim, D.,Oll√©, E. P., Oravec, J., Wetter, M., Vrabie, D. L., and Helsen, L. All you need to know about model predictive control for buildings. Annual Reviews in Control 50 (2020), 190‚Äì232. https://doi.org/10.1016/j.arcontrol.2020.09.001*\n",
        "\n",
        "- **[4]** *V√°zquez-Canteli, J. R., and Nagy, Z. Reinforcement learning\n",
        "for demand response: A review of algorithms and modeling techniques.\n",
        "Applied energy 235 (2019), 1072‚Äì1089. https://doi.org/10.1016/j.apenergy.2018.11.002*\n",
        "\n",
        "- **[5]** *Chen, B., Cai, Z., and Berg√©s, M. Gnu-RL: A Practical and Scalable Reinforcement Learning Solution for Building HVAC Control Using a Differentiable MPC Policy. Frontiers in Built Environment 6 (2020). https://doi.org/10.3389/fbuil.2020.562239*\n",
        "\n",
        "- **[6]** *Sutton, R. S., and Barto, A. G. Reinforcement Learning: An Introduction, second ed. The MIT Press, 2018.*\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}